{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1fd2864",
   "metadata": {},
   "source": [
    "## Example of optimization done by PyTorch compared to the NumPy library (JIT compilation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c67e579",
   "metadata": {},
   "source": [
    "#### JIT Compilation and Caching ( using only CPU)\n",
    "One significant optimization that PyTorch implements is Just-In-Time (JIT) compilation and caching, which can lead to performance improvements over NumPy for certain operations.\n",
    "\n",
    "In this example, we define a simple computation and run it using NumPy, PyTorch, and PyTorch with JIT compilation. The JIT-compiled version often shows better performance, especially for repeated computations, due to the following optimizations:\n",
    "\n",
    "    - Code optimization: The JIT compiler analyzes the computation graph and applies various optimizations, such as operator fusion and dead code elimination.\n",
    "    - Specialized code generation: It generates specialized machine code for the specific operations and data types used in your function.\n",
    "    - Caching: Once compiled, the optimized version is cached, reducing overhead for subsequent calls.\n",
    "    - Reduced Python overhead: JIT compilation can reduce the Python interpreter overhead by executing more operations in compiled code.\n",
    "\n",
    "These optimizations can lead to significant performance improvements, especially for complex operations or when the same computation is repeated many times. The exact performance gain can vary depending on the specific operation, input sizes, and hardware. \n",
    "\n",
    "It's worth noting that for very simple operations or small data sizes, the overhead of JIT compilation might outweigh its benefits. However, for larger, more complex computations typical in deep learning scenarios, PyTorch's JIT optimization can provide substantial speedups compared to NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a021d87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Define a simple function\n",
    "def compute(x, y):\n",
    "    return x * y + x\n",
    "\n",
    "# Create input data\n",
    "x = np.random.rand(10000, 10000)\n",
    "y = np.random.rand(10000, 10000)\n",
    "\n",
    "# NumPy version\n",
    "def numpy_compute():\n",
    "    print('Inside numpy_compute()')\n",
    "    return compute(x, y)\n",
    "\n",
    "# PyTorch version\n",
    "def torch_compute():\n",
    "    print('Inside torch_compute()')\n",
    "    x_torch = torch.from_numpy(x)\n",
    "    y_torch = torch.from_numpy(y)\n",
    "    return compute(x_torch, y_torch)\n",
    "\n",
    "# JIT-compiled PyTorch version\n",
    "@torch.jit.script\n",
    "def torch_jit_compute(x, y):\n",
    "    print('Inside torch_jit_compute()')\n",
    "    return compute(x, y)\n",
    "\n",
    "def torch_jit_run():\n",
    "    x_torch = torch.from_numpy(x)\n",
    "    y_torch = torch.from_numpy(y)\n",
    "    print('Inside torch_jit_run()')\n",
    "    return torch_jit_compute(x_torch, y_torch)\n",
    "\n",
    "# Benchmark\n",
    "def benchmark(func, name):\n",
    "    start = time.time()\n",
    "    for _ in range(1000):\n",
    "        func()\n",
    "    end = time.time()\n",
    "    print(f\"{name} took {end - start:.4f} seconds\")\n",
    "\n",
    "benchmark(numpy_compute, \"NumPy\")\n",
    "benchmark(torch_compute, \"PyTorch\")\n",
    "benchmark(torch_jit_run, \"PyTorch JIT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436274cd",
   "metadata": {},
   "source": [
    "NumPy took 339.1130 seconds\n",
    "\n",
    "PyTorch took 137.8524 seconds\n",
    "\n",
    "PyTorch JIT took 69.9366 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ee429f",
   "metadata": {},
   "source": [
    "### JIT Compilation and Caching ( using GPU if available)\n",
    "#### (But with the  overhead of transferring data between CPU and GPU memory)\n",
    "\n",
    "**Key changes from the above version:\n",
    "\n",
    "    - We use torch.device() to automatically select GPU if available, otherwise fallback to CPU.\n",
    "    - The input data is cast to float32 to ensure compatibility with GPU operations.\n",
    "    - In the PyTorch functions, we move the tensors to the selected device using .to(device).\n",
    "    - We add a GPU warm-up step to ensure the GPU is initialized before benchmarking.\n",
    "    - We use torch.cuda.synchronize() before and after the warm-up to ensure all CUDA operations are completed.\n",
    "    - In the PyTorch functions, we move the result back to CPU and convert to numpy array if GPU was used. This ensures a fair comparison with the NumPy version, which always returns a numpy array.\n",
    "    - The JIT-compiled function is now device-agnostic. It will run on whatever device the input tensors are on.\n",
    "\n",
    "This version will use the GPU if it's available, potentially showing even more significant performance improvements for PyTorch over NumPy, especially for larger computations. However, for smaller computations, the overhead of moving data to and from the GPU might offset some of the performance gains. \n",
    "\n",
    "Remember that the relative performance can still vary based on the specific hardware, CUDA version, and the nature of the computation being performed. It's always a good idea to benchmark with your specific use case and data sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a946e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Starting benchmarks...\n",
      "NumPy took 176.4616 seconds\n",
      "PyTorch took 183.7095 seconds\n",
      "PyTorch JIT took 181.1943 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Check if GPU is available and set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define a simple function\n",
    "def compute(x, y):\n",
    "    return x * y + x\n",
    "\n",
    "# Create input data\n",
    "x = np.random.rand(10000, 10000).astype(np.float32)\n",
    "y = np.random.rand(10000, 10000).astype(np.float32)\n",
    "\n",
    "x_torch = torch.from_numpy(x).to(device)\n",
    "y_torch = torch.from_numpy(y).to(device)\n",
    "\n",
    "# NumPy version\n",
    "def numpy_compute():\n",
    "    return compute(x, y)\n",
    "\n",
    "# PyTorch version\n",
    "def torch_compute():\n",
    "    result = compute(x_torch, y_torch)\n",
    "    return result.cpu().numpy() if device.type == \"cuda\" else result.numpy()\n",
    "\n",
    "# JIT-compiled PyTorch version\n",
    "@torch.jit.script\n",
    "def torch_jit_compute(x, y):\n",
    "    return compute(x, y)\n",
    "\n",
    "def torch_jit_run():\n",
    "    result = torch_jit_compute(x_torch, y_torch)\n",
    "    return result.cpu().numpy() if device.type == \"cuda\" else result.numpy()\n",
    "\n",
    "# Benchmark\n",
    "def benchmark(func, name):\n",
    "    start = time.time()\n",
    "    for _ in range(1000):\n",
    "        func()\n",
    "    end = time.time()\n",
    "    print(f\"{name} took {end - start:.4f} seconds\")\n",
    "\n",
    "# Warm-up GPU\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.synchronize()\n",
    "    warm_up = torch.rand(10000, 10000, device=device)\n",
    "    warm_up = warm_up * warm_up + warm_up\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "print(\"Starting benchmarks...\")\n",
    "benchmark(numpy_compute, \"NumPy\")\n",
    "benchmark(torch_compute, \"PyTorch\")\n",
    "benchmark(torch_jit_run, \"PyTorch JIT\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d14d8473",
   "metadata": {},
   "source": [
    "## Modified version of the code that keeps the results on the GPU for PyTorch operations:\n",
    "\n",
    "Key changes and explanations:\n",
    "\n",
    "    - We create the PyTorch tensors directly on the GPU using device=device in the tensor creation.\n",
    "    - The mul function no longer explicitly calls .cuda() since the tensors are already on the GPU.\n",
    "    - We don't move the result back to CPU in the PyTorch function.\n",
    "    - In the benchmark function, we use torch.cuda.synchronize() for PyTorch operations to ensure all GPU computations are completed before measuring time.\n",
    "    - We run the benchmark for 10,000 iterations to get a more accurate measurement.\n",
    "    - For the NumPy benchmark, we move the PyTorch tensors to CPU and convert them to NumPy arrays only once, outside the timing loop.\n",
    "\n",
    "This setup should give you a more accurate comparison between GPU-accelerated PyTorch operations and CPU-based NumPy operations. The PyTorch operations now stay entirely on the GPU, **avoiding the overhead of transferring data between CPU and GPU memory** for each operation. \n",
    "\n",
    "Remember that for small tensor sizes like 200x100, the GPU might not show significant speedup due to the overhead of GPU kernel launches. The benefits of GPU acceleration are typically more pronounced for larger tensor sizes and more complex operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afc39a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Warming up GPU...\n",
      "Starting benchmarks...\n",
      "NumPy (CPU) took 184.981980 seconds\n",
      "PyTorch took 4.242224 seconds\n",
      "PyTorch JIT took 2.127695 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Check if GPU is available and set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define a simple function\n",
    "def compute(x, y):\n",
    "    return x * y + x\n",
    "\n",
    "# Create input data\n",
    "x = torch.rand(10000, 10000, device=device)\n",
    "y = torch.rand(10000, 10000, device=device)\n",
    "\n",
    "x_np = x.cpu().numpy()\n",
    "y_np = y.cpu().numpy()\n",
    "\n",
    "# NumPy version (always on CPU)\n",
    "def numpy_compute():\n",
    "    return compute(x_np, y_np)\n",
    "\n",
    "# PyTorch version\n",
    "def torch_compute():\n",
    "    return compute(x, y)\n",
    "\n",
    "# JIT-compiled PyTorch version\n",
    "@torch.jit.script\n",
    "def torch_jit_compute(x, y):\n",
    "    return compute(x, y)\n",
    "\n",
    "# Benchmark function\n",
    "def benchmark(func, name, *args):\n",
    "    start = time.time()\n",
    "    for _ in range(1000):\n",
    "        result = func(*args)\n",
    "        if torch.is_tensor(result):\n",
    "            torch.cuda.synchronize()  # Ensure GPU operations are completed\n",
    "    end = time.time()\n",
    "    print(f\"{name} took {end - start:.6f} seconds\")\n",
    "\n",
    "# Warm-up GPU\n",
    "if device.type == \"cuda\":\n",
    "    warm_up = torch.rand(10000, 10000, device=device)\n",
    "    print(\"Warming up GPU...\")\n",
    "    warm_up = warm_up * warm_up + warm_up\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "print(\"Starting benchmarks...\")\n",
    "\n",
    "# NumPy benchmark\n",
    "benchmark(numpy_compute, \"NumPy (CPU)\")\n",
    "\n",
    "# PyTorch benchmark\n",
    "benchmark(torch_compute, \"PyTorch\")\n",
    "\n",
    "# PyTorch JIT benchmark\n",
    "benchmark(torch_jit_compute, \"PyTorch JIT\", x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cdb6f2",
   "metadata": {},
   "source": [
    "# Key Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85e2a5c",
   "metadata": {},
   "source": [
    "We have seen :\n",
    " \n",
    "\n",
    "\n",
    "Here are some key strategies to optimize overhead due to data transfer between GPU and CPU in deep learning projects:\n",
    "\n",
    "    - Use Pinned Memory:\n",
    "    When transferring data from CPU to GPU, use pinned (page-locked) memory. This can be done by setting pin_memory=True in DataLoader or using torch.cuda.FloatTensor(x).pin_memory() for tensors\n",
    "    - Asynchronous Data Transfer:\n",
    "    Utilize asynchronous data transfers to overlap computation with data transfer. This can be achieved using CUDA streams\n",
    "    - Minimize Transfer Frequency:\n",
    "    Try to keep data on the GPU as much as possible. Only transfer data to CPU when absolutely necessary, such as for logging or saving checkpoints\n",
    "    - Batch Processing:\n",
    "    Process data in batches to reduce the number of transfers and take advantage of GPU parallelism\n",
    "    - Use CUDA Graphs:\n",
    "    For static computational graphs, use CUDA graphs to reduce CPU overhead in launching GPU operations\n",
    "    - Avoid Unnecessary Synchronization:\n",
    "    Minimize calls to torch.cuda.synchronize() and other operations that force synchronization between CPU and GPU\n",
    "    - Direct Tensor Creation on GPU:\n",
    "    Create tensors directly on the GPU when possible, rather than creating them on CPU and then transferring\n",
    "    - Optimize Data Loading:\n",
    "    Use efficient data loading techniques like NVIDIA DALI or PyTorch's DataLoader with multiple workers and pinned memory\n",
    "    - Consider Using SpeedTorch:\n",
    "    For specific use cases, libraries like SpeedTorch can provide faster CPU-GPU data transfer than standard PyTorch methods1\n",
    "    - Profile and Benchmark:\n",
    "    Use tools like PyTorch Profiler or NVIDIA Nsight Systems to identify bottlenecks in data transfer and optimize accordingly\n",
    "    - Use Appropriate Data Types:\n",
    "    Ensure you're using the most appropriate data types for your tensors to minimize transfer sizes\n",
    "    - Pipeline Parallelism:\n",
    "    For large models, consider using pipeline parallelism to distribute computation across multiple GPUs and reduce data transfer overhead4\n",
    "    - Avoid Unnecessary Transfers:\n",
    "    Be cautious about operations that implicitly move data between devices, such as using .item() or .cpu() in critical paths\n",
    "\n",
    "Remember that the effectiveness of these strategies can vary depending on your specific use case and hardware setup. It's important to profile your application and focus on optimizing the most significant bottlenecks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925371b8",
   "metadata": {},
   "source": [
    "## Profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f2249b",
   "metadata": {},
   "source": [
    "To profile a deep learning-based application for breast cancer diagnosis, you can use several techniques and tools. Here's a comprehensive approach:\n",
    "\n",
    "1. Use PyTorch Profiler:\n",
    "PyTorch provides a built-in profiler that can help identify performance bottlenecks:\n",
    "\n",
    "```python\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], \n",
    "             record_shapes=True, profile_memory=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        model(input)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "```\n",
    "\n",
    "This will give you detailed information about CPU and GPU usage, memory consumption, and execution time for different operations[4].\n",
    "\n",
    "2. NVIDIA Nsight Systems:\n",
    "For GPU-specific profiling, NVIDIA Nsight Systems can provide detailed insights into GPU utilization, memory transfers, and kernel execution[4].\n",
    "\n",
    "3. Memory Profiling:\n",
    "Use `torch.cuda.memory_summary()` to get a detailed breakdown of GPU memory usage[4].\n",
    "\n",
    "4. Data Loading Optimization:\n",
    "Profile your data loading pipeline:\n",
    "- Use `num_workers` in DataLoader to parallelize data loading\n",
    "- Set `pin_memory=True` for faster CPU to GPU transfers[4]\n",
    "\n",
    "5. Model Architecture Analysis:\n",
    "Use `torchinfo` or `summary` to get a detailed view of your model architecture, including the number of parameters and FLOPs[4].\n",
    "\n",
    "6. Batch Size and Learning Rate Profiling:\n",
    "Experiment with different batch sizes and learning rates to find the optimal balance between speed and accuracy[4].\n",
    "\n",
    "7. Mixed Precision Training:\n",
    "Profile the performance impact of using mixed precision training with `torch.cuda.amp`[4].\n",
    "\n",
    "8. Distributed Training Profiling:\n",
    "If using distributed training, profile the communication overhead and load balancing between GPUs[4].\n",
    "\n",
    "9. Inference Optimization:\n",
    "For deployment, profile the inference speed and consider optimizations like model quantization or TorchScript[4].\n",
    "\n",
    "10. Custom Profiling:\n",
    "Implement custom timing functions for specific parts of your pipeline:\n",
    "\n",
    "```python\n",
    "import time\n",
    "\n",
    "def timeit(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end = time.time()\n",
    "        print(f\"{func.__name__} took {end - start:.2f} seconds\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "@timeit\n",
    "def preprocess_data(data):\n",
    "    # Your preprocessing code here\n",
    "    pass\n",
    "```\n",
    "\n",
    "11. System-level Profiling:\n",
    "Use tools like `nvidia-smi` for GPU monitoring and `htop` for CPU and memory usage[4].\n",
    "\n",
    "12. End-to-end Profiling:\n",
    "Profile the entire pipeline from data loading to model inference to results analysis to identify overall bottlenecks[4].\n",
    "\n",
    "Remember to profile on a representative dataset and under realistic conditions. Also, consider the trade-offs between speed and accuracy, especially in a medical diagnosis context where accuracy is crucial. Always validate any optimizations to ensure they don't negatively impact the model's diagnostic performance.\n",
    "\n",
    "Citations:\n",
    "[1] https://jeas.springeropen.com/articles/10.1186/s44147-024-00411-z\n",
    "[2] https://discuss.pytorch.org/t/gpu-cpu-memory-transfer-time-changes/176384\n",
    "[3] https://discuss.pytorch.org/t/torch-is-slow-compared-to-numpy/117502\n",
    "[4] https://towardsdatascience.com/optimize-pytorch-performance-for-speed-and-memory-efficiency-2022-84f453916ea6?gi=be1659ebf739\n",
    "[5] https://discuss.pytorch.org/t/does-jit-makes-model-faster/44532\n",
    "[6] https://discuss.pytorch.org/t/cpu-x10-faster-than-gpu-recommendations-for-gpu-implementation-speed-up/54980\n",
    "[7] https://www.mdpi.com/2076-3417/13/21/11654\n",
    "[8] https://discuss.pytorch.org/t/pytorch-tensor-performance-vs-numpy-array/9168"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
