{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Deep Learning Forecasting model with PyTorch\n",
    "In this notebook, you will create a Recurrent Neural Network (RNN) that you can use to forecast the battery cycles used for time series battery data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">1</span><span class=\"ansired\">]: </span>[&apos;PyPI:(torch)-(empty)-(empty)-(empty)&apos;]</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Install the necessary libraries directly into the notebook context\n",
    "\n",
    "dbutils.library.installPyPI('torch')\n",
    "dbutils.library.restartPython()\n",
    "dbutils.library.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">1</span><span class=\"ansired\">]: </span>&lt;torch._C.Generator at 0x7f6a28f6c8b0&gt;</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import uuid\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "np.random.seed(1) # ensure repeatability\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "pd.set_option('display.max_columns', 10)\n",
    "\n",
    "torch.manual_seed(0) # ensure repeatability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the data\n",
    "\n",
    "The following cell will download the data set containing the daily battery time series.\n",
    "The CSV file will be saved in a temporary folder on the Databricks cluster for later reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Content files will be saved to /FileStore/CarBatteries_7ec54f66-baed-4639-80b4-15d97242775b\n",
       "wget -O /dbfs/FileStore/CarBatteries_7ec54f66-baed-4639-80b4-15d97242775b/daily-battery-time-series-v2.csv https://databricksdemostore.blob.core.windows.net/data/connected-car/daily-battery-time-series-v2.csv\n",
       "System resturned value 0\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a temporary folder to store locally relevant content for this notebook\n",
    "tempFolderName = '/FileStore/CarBatteries_{0}'.format(uuid.uuid4())\n",
    "dbutils.fs.mkdirs(tempFolderName)\n",
    "print('Content files will be saved to {0}'.format(tempFolderName))\n",
    "\n",
    "filesToDownload = ['daily-battery-time-series-v2.csv']\n",
    "\n",
    "for fileToDownload in filesToDownload:\n",
    "  downloadCommand = 'wget -O ''/dbfs{0}/{1}'' ''https://databricksdemostore.blob.core.windows.net/data/connected-car/{1}'''.format(tempFolderName, fileToDownload)\n",
    "  print(downloadCommand)\n",
    "  ret_val = os.system(downloadCommand)\n",
    "  print('System resturned value %s' % (ret_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple check to make sure our data file was correctly downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">3</span><span class=\"ansired\">]: </span>[FileInfo(path=&apos;dbfs:/FileStore/CarBatteries_7ec54f66-baed-4639-80b4-15d97242775b/daily-battery-time-series-v2.csv&apos;, name=&apos;daily-battery-time-series-v2.csv&apos;, size=132341)]</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check that all files are successfully downloaded\n",
    "dbutils.fs.ls(tempFolderName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "\n",
    "The previously downloaded CSV file will be loaded into a Pandas Dataframe and its first few rows inspected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-width:1500px;overflow:auto;\">\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Battery_ID</th>\n",
       "      <th>Battery_Age_Days</th>\n",
       "      <th>Number_Of_Trips</th>\n",
       "      <th>Daily_Trip_Duration</th>\n",
       "      <th>Daily_Cycles_Used</th>\n",
       "      <th>Lifetime_Cycles_Used</th>\n",
       "      <th>Battery_Rated_Cycles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>67.845608</td>\n",
       "      <td>0.166920</td>\n",
       "      <td>0.166920</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>53.450798</td>\n",
       "      <td>0.131505</td>\n",
       "      <td>0.298425</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-03</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>58.841433</td>\n",
       "      <td>0.144767</td>\n",
       "      <td>0.443193</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>60.638403</td>\n",
       "      <td>0.149188</td>\n",
       "      <td>0.592381</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-05</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>62.646910</td>\n",
       "      <td>0.154130</td>\n",
       "      <td>0.746511</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "file_path = '/dbfs%s/%s' % (tempFolderName, filesToDownload[0])\n",
    "df = pd.read_csv(file_path, delimiter=',')\n",
    "df = df[['Date','Battery_ID','Battery_Age_Days','Number_Of_Trips','Daily_Trip_Duration','Daily_Cycles_Used', 'Lifetime_Cycles_Used', 'Battery_Rated_Cycles']]\n",
    "\n",
    "# Remove the temporary folder\n",
    "dbutils.fs.rm(tempFolderName, recurse=True)\n",
    "\n",
    "# Inspect the data frame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The time series related to one specific Battery_ID will be isolated and its shape checked. To keep the model simple and make is easier to understand, only one column will be used - Daily_Cycles_Used.\n",
    "\n",
    "In case the dataset contains more time series, the process of training and prediction must be repeated for each individual series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">5</span><span class=\"ansired\">]: </span>(1539, 1)</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Isolate the time series related to one Battery_ID\n",
    "df_source = df[df['Battery_ID'] == 0][['Daily_Cycles_Used']]\n",
    "df_source.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape our input as a one dimensional array. This will make some of the operations we'll perform easier to follow and understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "source = df_source.values.reshape(1, df_source.shape[0])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data\n",
    "\n",
    "Our time series prediction model will use a special kind of RNN (Recurrent Neural Network) built out of LSTM (Long Short Term Memory) cells. LSTMs are not particularly happy with very long series so we are setting the maximum limit of a time series to 250 steps (```sample_size```). Based on this value, we calculate then the maximum number of non-overlapping samples we can get from our original time series (```num_samples```).\n",
    "\n",
    "We then consolidate these samples into two matrixes, ```input``` and ```output```. Notice they are built in a way that for every element Xn in every sample in ```input```, the corresponding element from ```output``` is equal to the one that follows Xn (which is Xn+1). The fundamental idea is that we're looking to train a model that will be capable of predicting Xn+1 based on Xn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_size = 250\n",
    "num_samples = source.shape[0] // sample_size\n",
    "\n",
    "input = np.zeros((num_samples, sample_size))\n",
    "output = np.zeros((num_samples, sample_size))\n",
    "\n",
    "for i in range(num_samples):\n",
    "  input[i] = source[-(i+1) * sample_size - 2 : -i * sample_size - 2]\n",
    "  output[i] = source[-(i+1) * sample_size - 1 : -i * sample_size - 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are using PyTorch, we're moving ```input``` and ```output``` into tensor space. The \\_t notation is used to identify a variable that is a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">torch.Size([6, 250])\n",
       "torch.Size([6, 250])\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_t = torch.from_numpy(input)\n",
    "target_t = torch.from_numpy(output)\n",
    "print(input_t.shape)\n",
    "print(target_t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and train the model\n",
    "\n",
    "Next, we define our model as a class derived from the base class ```nn.Module```. Our model contains two hidden LSTM layers of sizes ```hidden_layer1_size``` and ```hidden_layer2_size``` respectively. The output of the second LSTM layer is fed into a linear layer that will combine all components into a single output.\n",
    "\n",
    "Each hidden layer also needs a pair of variables to hold internal state (```h_t```,```c_t``` and ```h_t2```,```c_t2``` respectively). They are used by the LSTM cells to keep track of their \"memory\" during the run of every epoch (implemented by the ```forward``` method). Notice how the internal state is reset at the beginning of each epoch run.\n",
    "\n",
    "Also notice the ```future``` parameter which controls whether we want to also make predictions into the future or not. The value of this parameter will be 0 during the training process and set to a number of days when the model is called to make a prediction once it is trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The number of nodes in the hidden layers\n",
    "hidden_layer1_size = 75\n",
    "hidden_layer2_size = 75\n",
    "\n",
    "class LSTMPredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTMPredictor, self).__init__()\n",
    "        self.lstm1 = nn.LSTMCell(1, hidden_layer1_size)\n",
    "        self.lstm2 = nn.LSTMCell(hidden_layer1_size, hidden_layer2_size)\n",
    "        self.linear = nn.Linear(hidden_layer2_size, 1)\n",
    "\n",
    "    def forward(self, input, future = 0):\n",
    "        outputs = []\n",
    "        h_t = torch.zeros(input.size(0), hidden_layer1_size, dtype=torch.double)\n",
    "        c_t = torch.zeros(input.size(0), hidden_layer1_size, dtype=torch.double)\n",
    "        h_t2 = torch.zeros(input.size(0), hidden_layer2_size, dtype=torch.double)\n",
    "        c_t2 = torch.zeros(input.size(0), hidden_layer2_size, dtype=torch.double)\n",
    "\n",
    "        for i, input_t in enumerate(input.chunk(input.size(1), dim=1)):\n",
    "            h_t, c_t = self.lstm1(input_t, (h_t, c_t))\n",
    "            h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2))\n",
    "            output = self.linear(h_t2)\n",
    "            outputs += [output]\n",
    "        for i in range(future):# if we should predict the future\n",
    "            h_t, c_t = self.lstm1(output, (h_t, c_t))\n",
    "            h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2))\n",
    "            output = self.linear(h_t2)\n",
    "            outputs += [output]\n",
    "        outputs = torch.stack(outputs, 1).squeeze(2)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the number of epochs, the learning rate, the method to calculate the loss function, and the optimizer used for the backwards pass on the network during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Increase the number of epochs for better results\n",
    "epochs = 150\n",
    "learning_rate = 0.5\n",
    "\n",
    "# build the model\n",
    "pred = LSTMPredictor()\n",
    "pred.double() #convert all internal values to doubles\n",
    "criteria = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(pred.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the actual training on the model.\n",
    "\n",
    "For each epoch, we will perform the following steps:\n",
    "\n",
    "- Make a prediction using the ```input_t``` input tensor\n",
    "- Calculate how far is the predicted result from the expected result (stored in the ```target_t``` tensor). The distance is given by the value of the loss function, which we also save.\n",
    "- Zero out the gradients (we are resetting them on each epoch)\n",
    "- Trigger the backpropagation process through which we are recalibrating the internal weights of the network\n",
    "- Activate the optimizer to help the recalibration process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Starting epoch 1...\n",
       "Current loss: 0.021775809198469307\n",
       "Starting epoch 2...\n",
       "Current loss: 471.2960242948518\n",
       "Starting epoch 3...\n",
       "Current loss: 52.097640831602725\n",
       "Starting epoch 4...\n",
       "Current loss: 53.6799114008337\n",
       "Starting epoch 5...\n",
       "Current loss: 234.56473798341688\n",
       "Starting epoch 6...\n",
       "Current loss: 217.4136138260876\n",
       "Starting epoch 7...\n",
       "Current loss: 34.9357771709064\n",
       "Starting epoch 8...\n",
       "Current loss: 3.019658577680696\n",
       "Starting epoch 9...\n",
       "Current loss: 0.25136909526042883\n",
       "Starting epoch 10...\n",
       "Current loss: 1.1700782756376753\n",
       "Starting epoch 11...\n",
       "Current loss: 0.002944327185952843\n",
       "Starting epoch 12...\n",
       "Current loss: 0.02120753290484105\n",
       "Starting epoch 13...\n",
       "Current loss: 0.05175410549508887\n",
       "Starting epoch 14...\n",
       "Current loss: 0.08981591144032687\n",
       "Starting epoch 15...\n",
       "Current loss: 0.13155633755194512\n",
       "Starting epoch 16...\n",
       "Current loss: 0.17390612232237276\n",
       "Starting epoch 17...\n",
       "Current loss: 0.21445608970998478\n",
       "Starting epoch 18...\n",
       "Current loss: 0.25137075839361833\n",
       "Starting epoch 19...\n",
       "Current loss: 0.2833152833440504\n",
       "Starting epoch 20...\n",
       "Current loss: 0.30939107689168194\n",
       "Starting epoch 21...\n",
       "Current loss: 0.3290772652327771\n",
       "Starting epoch 22...\n",
       "Current loss: 0.34217630107793745\n",
       "Starting epoch 23...\n",
       "Current loss: 0.34876281524133396\n",
       "Starting epoch 24...\n",
       "Current loss: 0.3491352902607125\n",
       "Starting epoch 25...\n",
       "Current loss: 0.34377046290746527\n",
       "Starting epoch 26...\n",
       "Current loss: 0.3332805632874005\n",
       "Starting epoch 27...\n",
       "Current loss: 0.31837361119127455\n",
       "Starting epoch 28...\n",
       "Current loss: 0.2998170393433731\n",
       "Starting epoch 29...\n",
       "Current loss: 0.2784049152830253\n",
       "Starting epoch 30...\n",
       "Current loss: 0.2549290015370592\n",
       "Starting epoch 31...\n",
       "Current loss: 0.2301538374164846\n",
       "Starting epoch 32...\n",
       "Current loss: 0.20479595319564803\n",
       "Starting epoch 33...\n",
       "Current loss: 0.17950724523327305\n",
       "Starting epoch 34...\n",
       "Current loss: 0.15486245435584436\n",
       "Starting epoch 35...\n",
       "Current loss: 0.13135060427855721\n",
       "Starting epoch 36...\n",
       "Current loss: 0.10937017598024412\n",
       "Starting epoch 37...\n",
       "Current loss: 0.0892277210848553\n",
       "Starting epoch 38...\n",
       "Current loss: 0.07113955508397504\n",
       "Starting epoch 39...\n",
       "Current loss: 0.05523612166631631\n",
       "Starting epoch 40...\n",
       "Current loss: 0.0415685838639429\n",
       "Starting epoch 41...\n",
       "Current loss: 0.030117176912284195\n",
       "Starting epoch 42...\n",
       "Current loss: 0.020800851770021063\n",
       "Starting epoch 43...\n",
       "Current loss: 0.013487746693184258\n",
       "Starting epoch 44...\n",
       "Current loss: 0.008006046110513584\n",
       "Starting epoch 45...\n",
       "Current loss: 0.004154819842084808\n",
       "Starting epoch 46...\n",
       "Current loss: 0.0017144795914006534\n",
       "Starting epoch 47...\n",
       "Current loss: 0.0004565414803103086\n",
       "Starting epoch 48...\n",
       "Current loss: 0.00015244085354047395\n",
       "Starting epoch 49...\n",
       "Current loss: 0.0005812062392574989\n",
       "Starting epoch 50...\n",
       "Current loss: 0.0015358608209278032\n",
       "Starting epoch 51...\n",
       "Current loss: 0.0028284797840690897\n",
       "Starting epoch 52...\n",
       "Current loss: 0.00429388839213348\n",
       "Starting epoch 53...\n",
       "Current loss: 0.005792036848474397\n",
       "Starting epoch 54...\n",
       "Current loss: 0.0072091324890712835\n",
       "Starting epoch 55...\n",
       "Current loss: 0.008457646576128193\n",
       "Starting epoch 56...\n",
       "Current loss: 0.009475341274559386\n",
       "Starting epoch 57...\n",
       "Current loss: 0.010223482033172061\n",
       "Starting epoch 58...\n",
       "Current loss: 0.010684411673044528\n",
       "Starting epoch 59...\n",
       "Current loss: 0.010858665455275343\n",
       "Starting epoch 60...\n",
       "Current loss: 0.01076180199359038\n",
       "Starting epoch 61...\n",
       "Current loss: 0.010421114058218637\n",
       "Starting epoch 62...\n",
       "Current loss: 0.00987236721699559\n",
       "Starting epoch 63...\n",
       "Current loss: 0.009156694111894738\n",
       "Starting epoch 64...\n",
       "Current loss: 0.008317749248758539\n",
       "Starting epoch 65...\n",
       "Current loss: 0.007399204741778851\n",
       "Starting epoch 66...\n",
       "Current loss: 0.00644264268949029\n",
       "Starting epoch 67...\n",
       "Current loss: 0.005485875839450441\n",
       "Starting epoch 68...\n",
       "Current loss: 0.0045617058501289595\n",
       "Starting epoch 69...\n",
       "Current loss: 0.003697108533827679\n",
       "Starting epoch 70...\n",
       "Current loss: 0.002912818529287357\n",
       "Starting epoch 71...\n",
       "Current loss: 0.002223272280112227\n",
       "Starting epoch 72...\n",
       "Current loss: 0.0016368581698311608\n",
       "Starting epoch 73...\n",
       "Current loss: 0.0011564161942255572\n",
       "Starting epoch 74...\n",
       "Current loss: 0.0007799264862893593\n",
       "Starting epoch 75...\n",
       "Current loss: 0.0005013260642984685\n",
       "Starting epoch 76...\n",
       "Current loss: 0.00031139595747331624\n",
       "Starting epoch 77...\n",
       "Current loss: 0.0001986659076807175\n",
       "Starting epoch 78...\n",
       "Current loss: 0.00015029063313542855\n",
       "Starting epoch 79...\n",
       "Current loss: 0.000152859636327514\n",
       "Starting epoch 80...\n",
       "Current loss: 0.00019311121709820038\n",
       "Starting epoch 81...\n",
       "Current loss: 0.000258530218702941\n",
       "Starting epoch 82...\n",
       "Current loss: 0.0003378176473134423\n",
       "Starting epoch 83...\n",
       "Current loss: 0.0004212282878824149\n",
       "Starting epoch 84...\n",
       "Current loss: 0.0005007794926253213\n",
       "Starting epoch 85...\n",
       "Current loss: 0.0005703402257143993\n",
       "Starting epoch 86...\n",
       "Current loss: 0.0006256140751593603\n",
       "Starting epoch 87...\n",
       "Current loss: 0.0006640332357518575\n",
       "Starting epoch 88...\n",
       "Current loss: 0.000684582443377929\n",
       "Starting epoch 89...\n",
       "Current loss: 0.0006875725817329056\n",
       "Starting epoch 90...\n",
       "Current loss: 0.0006743833193288599\n",
       "Starting epoch 91...\n",
       "Current loss: 0.0006471928376135879\n",
       "Starting epoch 92...\n",
       "Current loss: 0.0006087106747849816\n",
       "Starting epoch 93...\n",
       "Current loss: 0.0005619271410797011\n",
       "Starting epoch 94...\n",
       "Current loss: 0.0005098898663401182\n",
       "Starting epoch 95...\n",
       "Current loss: 0.0004555150151621465\n",
       "Starting epoch 96...\n",
       "Current loss: 0.00040143772506511134\n",
       "Starting epoch 97...\n",
       "Current loss: 0.0003499035389497199\n",
       "Starting epoch 98...\n",
       "Current loss: 0.0003027001340862506\n",
       "Starting epoch 99...\n",
       "Current loss: 0.00026112658258014773\n",
       "Starting epoch 100...\n",
       "Current loss: 0.00022599576610396336\n",
       "Starting epoch 101...\n",
       "Current loss: 0.00019766443241179686\n",
       "Starting epoch 102...\n",
       "Current loss: 0.00017608471586498634\n",
       "Starting epoch 103...\n",
       "Current loss: 0.00016087071751272024\n",
       "Starting epoch 104...\n",
       "Current loss: 0.0001513739012793976\n",
       "Starting epoch 105...\n",
       "Current loss: 0.00014676154645974226\n",
       "Starting epoch 106...\n",
       "Current loss: 0.0001460932292805858\n",
       "Starting epoch 107...\n",
       "Current loss: 0.0001483912105767009\n",
       "Starting epoch 108...\n",
       "Current loss: 0.0001527016067682179\n",
       "Starting epoch 109...\n",
       "Current loss: 0.00015814424679386923\n",
       "Starting epoch 110...\n",
       "Current loss: 0.0001639501065004629\n",
       "Starting epoch 111...\n",
       "Current loss: 0.00016948611316382775\n",
       "Starting epoch 112...\n",
       "Current loss: 0.00017426788756201073\n",
       "Starting epoch 113...\n",
       "Current loss: 0.00017796161338558085\n",
       "Starting epoch 114...\n",
       "Current loss: 0.0001803766803196021\n",
       "Starting epoch 115...\n",
       "Current loss: 0.0001814510359866611\n",
       "Starting epoch 116...\n",
       "Current loss: 0.00018123131126856367\n",
       "Starting epoch 117...\n",
       "Current loss: 0.00017984976965940838\n",
       "Starting epoch 118...\n",
       "Current loss: 0.0001774999966386943\n",
       "Starting epoch 119...\n",
       "Current loss: 0.00017441301587087983\n",
       "Starting epoch 120...\n",
       "Current loss: 0.00017083522339934195\n",
       "Starting epoch 121...\n",
       "Current loss: 0.00016700919682815303\n",
       "Starting epoch 122...\n",
       "Current loss: 0.00016315808990294408\n",
       "Starting epoch 123...\n",
       "Current loss: 0.00015947398693391286\n",
       "Starting epoch 124...\n",
       "Current loss: 0.00015611028511839015\n",
       "Starting epoch 125...\n",
       "Current loss: 0.00015317791036304732\n",
       "Starting epoch 126...\n",
       "Current loss: 0.00015074496319018177\n",
       "Starting epoch 127...\n",
       "Current loss: 0.00014883924053402708\n",
       "Starting epoch 128...\n",
       "Current loss: 0.00014745298715723143\n",
       "Starting epoch 129...\n",
       "Current loss: 0.000146549193780201\n",
       "Starting epoch 130...\n",
       "Current loss: 0.0001460687715692285\n",
       "Starting epoch 131...\n",
       "Current loss: 0.0001459379859685968\n",
       "Starting epoch 132...\n",
       "Current loss: 0.00014607561726965335\n",
       "Starting epoch 133...\n",
       "Current loss: 0.00014639942056214513\n",
       "Starting epoch 134...\n",
       "Current loss: 0.00014683157380260628\n",
       "Starting epoch 135...\n",
       "Current loss: 0.00014730292047678204\n",
       "Starting epoch 136...\n",
       "Current loss: 0.00014775592483286002\n",
       "Starting epoch 137...\n",
       "Current loss: 0.00014814635663102445\n",
       "Starting epoch 138...\n",
       "Current loss: 0.0001484438042783305\n",
       "Starting epoch 139...\n",
       "Current loss: 0.0001486311773854618\n",
       "Starting epoch 140...\n",
       "Current loss: 0.00014870340119437842\n",
       "Starting epoch 141...\n",
       "Current loss: 0.00014866552650515967\n",
       "Starting epoch 142...\n",
       "Current loss: 0.00014853048145756787\n",
       "Starting epoch 143...\n",
       "Current loss: 0.0001483166785315216\n",
       "Starting epoch 144...\n",
       "Current loss: 0.00014804566478211133\n",
       "Starting epoch 145...\n",
       "Current loss: 0.00014773996928948575\n",
       "Starting epoch 146...\n",
       "Current loss: 0.0001474212627690821\n",
       "Starting epoch 147...\n",
       "Current loss: 0.0001471089037066966\n",
       "Starting epoch 148...\n",
       "Current loss: 0.00014681890627769097\n",
       "Starting epoch 149...\n",
       "Current loss: 0.00014656333013112998\n",
       "Starting epoch 150...\n",
       "Current loss: 0.0001463500626677071\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses = []\n",
    "\n",
    "for epoch in np.arange(1, epochs + 1):\n",
    "    \n",
    "    print('Starting epoch %s...' % (epoch))\n",
    "    \n",
    "    # Feed the input through the network\n",
    "    out = pred(input_t)\n",
    "\n",
    "    # Calculate loss tensor\n",
    "    loss = criteria(out, target_t)\n",
    "    losses += [loss.item()]\n",
    "    print('Current loss: %s' % (loss.item()))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Trigger backpropagation\n",
    "    loss.backward()\n",
    "    # Move on\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the evolution of the loss function. We would expect the graph to flatline after a few initial pulses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(30,10))\n",
    "plt.title('The evolution of the LOSS function during training', fontsize=10)\n",
    "plt.xlabel('x', fontsize=20)\n",
    "plt.ylabel('y', fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "\n",
    "plt.plot(np.arange(epochs), losses, 'r', linewidth=1.0)\n",
    "\n",
    "display(fig)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the future\n",
    "\n",
    "One the training process is finished, we are using the trained model to predict the values for the next 30 days. Since our sample size in ```sample_size``` we are just taking the last ```sample_size``` elements from the original time series and feed them to the model.\n",
    "\n",
    "Notice the ```with torch.no_grad()``` option which basically tells PyTorch this is not part of any training process, hence there is no need to track the gradients on the tensors involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The model is trained, predict the next 30 days\n",
    "days_to_predict = 30\n",
    "\n",
    "# Get the tensor with the last sample_size values\n",
    "final_input = torch.from_numpy(source[-sample_size:].reshape(1, sample_size))\n",
    "\n",
    "# No need to track gradient anymore\n",
    "with torch.no_grad():\n",
    "    \n",
    "    y_t = pred(final_input, future=days_to_predict)\n",
    "    y = y_t.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of the prediction will contain the predicted output corresponding to the input plus a number of elements equal to the number of future days we need prediction for.\n",
    "\n",
    "We'll just take a look at the future values predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">[0.14263596 0.14263596 0.14263596 0.14263596 0.14263596 0.14263596\n",
       " 0.14263596 0.14263596 0.14263596 0.14263596 0.14263596 0.14263596\n",
       " 0.14263596 0.14263596 0.14263596 0.14263596 0.14263596 0.14263596\n",
       " 0.14263596 0.14263596 0.14263596 0.14263596 0.14263596 0.14263596\n",
       " 0.14263596 0.14263596 0.14263596 0.14263596 0.14263596 0.14263596]\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "future_predictions = y[0, - days_to_predict:]\n",
    "print(future_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(30,10))\n",
    "plt.title('Predict future values \\n(Red values are predicted values)', fontsize=10)\n",
    "plt.xlabel('x', fontsize=20)\n",
    "plt.ylabel('y', fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "\n",
    "plt.plot(np.arange(sample_size), source[-sample_size:], 'g', linewidth=1.0)\n",
    "plt.plot(np.arange(sample_size, sample_size + days_to_predict), future_predictions, 'r', linewidth=1.0)\n",
    "\n",
    "display(fig)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the last ```sample_size``` elements from the original time series in green and the predicted values for the next 30 days in red.\n",
    "\n",
    "Please note that we are using synthetic training data and the target value was randomly generated around a mean, thus you will observe that the predictions are closer to the mean of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "name": "Deep Learning with Time Series",
  "notebookId": 341510685844529
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
