{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a deep learning model\n",
    "In this notebook you will train a deep learning model to classify the descriptions of car components as compliant or non-compliant. You will train the model on the Azure Databricks cluster and use MLflow integration with Azure Machine Learning to track and log experiment metrics and artifacts in the Azure Machine Learning workspace.\n",
    "\n",
    "Each document in the supplied training data set is a short text description of the component as documented by an authorized technician. \n",
    "The contents include:\n",
    "- Manufacture year of the component (e.g. 1985, 2010)\n",
    "- Condition of the component (poor, fair, good, new)\n",
    "- Materials used in the component (plastic, carbon fiber, steel, iron)\n",
    "\n",
    "The compliance regulations dictate:\n",
    "*Any component manufactured before 1995 or in fair or poor condition or made with plastic or iron is out of compliance.*\n",
    "\n",
    "For example:\n",
    "* Manufactured in 1985 made of steel in fair condition -> **Non-compliant**\n",
    "* Good condition carbon fiber component manufactured in 2010 -> **Compliant**\n",
    "* Steel component manufactured in 1995 in fair condition -> **Non-Compliant**\n",
    "\n",
    "The labels present in this data are 0 for compliant, 1 for non-compliant.\n",
    "\n",
    "The challenge with classifying text data is that deep learning models only undertand vectors (e.g., arrays of numbers) and not text. To encode the car component descriptions as vectors, we use an algorithm from Stanford called [GloVe (Global Vectors for Word Representation)](https://nlp.stanford.edu/projects/glove/). GloVe provides us pre-trained vectors that we can use to convert a string of text into a vector.\n",
    "\n",
    "The model will be built using a type of DNN called the Long Short-Term Memory (LSTM) recurrent neural network using TensorFlow via the Keras library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">1</span><span class=\"ansired\">]: </span>[&apos;PyPI:(keras)-(empty)-(empty)-(empty)&apos;, &apos;PyPI:(azureml-mlflow)-(empty)-(empty)-(empty)&apos;, &apos;PyPI:(tensorflow)-(empty)-(empty)-(empty)&apos;, &apos;PyPI:(mlflow)-(empty)-(empty)-(empty)&apos;]</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Install the necessary libraries directly into the notebook context\n",
    "\n",
    "dbutils.library.installPyPI('tensorflow')\n",
    "dbutils.library.installPyPI('keras')\n",
    "dbutils.library.installPyPI('mlflow')\n",
    "dbutils.library.installPyPI('azureml-mlflow')\n",
    "dbutils.library.restartPython()\n",
    "dbutils.library.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to the Azure Machine Learning Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">azureml SDK version: 1.5.0\n",
       "mlflow version: 1.8.0\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import required packages\n",
    "\n",
    "import azureml\n",
    "from azureml.core import Run\n",
    "from azureml.core import Workspace\n",
    "from azureml.core.experiment import Experiment\n",
    "\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "\n",
    "print(\"azureml SDK version:\", azureml.core.VERSION)\n",
    "print(\"mlflow version:\", mlflow.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure access to the Azure Machine Learning resources\n",
    "To begin, you will need to provide the following information about your Azure Subscription.\n",
    "\n",
    "**If you are using your own Azure subscription, please provide names for subscription_id, resource_group, workspace_name and workspace_region to use.** Note that the workspace needs to be of type [Machine Learning Workspace](https://docs.microsoft.com/en-us/azure/machine-learning/service/setup-create-workspace).\n",
    "\n",
    "In the following cell, be sure to set the values for `subscription_id`, `resource_group`, `workspace_name` and `workspace_region` as directed by the comments (*these values can be acquired from the Azure Portal*).\n",
    "\n",
    "To get these values, do the following:\n",
    "1. Navigate to the Azure Portal and login with the credentials provided.\n",
    "2. From the left hand menu, under Favorites, select `Resource Groups`.\n",
    "3. In the list, select the resource group with the name similar to `MCW-AI-Lab`.\n",
    "4. From the Overview tab, capture the desired values.\n",
    "\n",
    "In addition to these, be sure to set the `experiment_name` with the name of the experiment you used in training the model with Automated Machine Learning.\n",
    "\n",
    "Execute the following cell by selecting the `>|Run` button in the command bar above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Provide the Subscription ID of your existing Azure subscription\n",
    "subscription_id = \"281b526e-0f57-4142-ae7c-b89b634fd26e\" # <- subscription you are using for this hands-on lab\n",
    "\n",
    "#Provide values for the existing Resource Group \n",
    "resource_group = \"MCW-AI-Lab\"\n",
    "\n",
    "#Provide the Workspace Name and Azure Region of the Azure Machine Learning Workspace\n",
    "workspace_name = \"AML-workspace-181384\"\n",
    "workspace_region = \"westus2\" # <- region of your resource group\n",
    "\n",
    "#Provide the name of the Automated ML experiment you executed previously\n",
    "experiment_name = \"Battery-Cycles\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cells to connect to your **Azure Machine Learning Workspace**\n",
    "\n",
    "**Important Note**: You will be prompted to login in the text that is output below the cell. Be sure to navigate to the URL displayed and enter the code that is provided. Once you have entered the code, return to this notebook and wait for the output to read `Workspace Provisioning complete`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Workspace Provisioning complete\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# By using the exist_ok param, if the worskpace already exists we get a reference to the existing workspace\n",
    "ws = Workspace.create(\n",
    "    name = \"AML-workspace-181384\",\n",
    "    subscription_id = \"281b526e-0f57-4142-ae7c-b89b634fd26e\",\n",
    "    resource_group = \"ODL-ml-181384\", \n",
    "    location = \"westus2\",\n",
    "    exist_ok = True)\n",
    "\n",
    "print(\"Workspace Provisioning complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Deep Learning model from text data\n",
    "The following cell will guide you through the process of preparing the data and using it to train a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">/databricks/python/lib/python3.7/site-packages/botocore/vendored/requests/packages/urllib3/_collections.py:1: DeprecationWarning: Using or importing the ABCs from &apos;collections&apos; instead of from &apos;collections.abc&apos; is deprecated, and in 3.8 it will stop working\n",
       "  from collections import Mapping, MutableMapping\n",
       "Using TensorFlow backend.\n",
       "pandas version: 0.23.4\n",
       "numpy version: 1.16.2\n",
       "matplotlib version: 3.0.3\n",
       "keras version: 2.3.1 tensorflow version: 2.2.0\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import required packages\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Embedding, Flatten, Dense, LSTM\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "print(\"pandas version:\", pd.__version__)\n",
    "print(\"numpy version:\", np.__version__)\n",
    "print(\"matplotlib version:\", matplotlib.__version__)\n",
    "print(\"keras version: {} tensorflow version: {}\".format(keras.__version__, tensorflow.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the GloVe embeddings to your environment.\n",
    "Run the following cell to download the embeddings to the `data` folder in your environment. Note: this may take a **few minutes** as the GloVe file is about 340 MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Downloading GloVe embeddings...\n",
       "Download complete.\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def download_glove():\n",
    "    print(\"Downloading GloVe embeddings...\")\n",
    "    import urllib.request\n",
    "    glove_url = ('https://quickstartsws9073123377.blob.core.windows.net/'\n",
    "                 'azureml-blobstore-0d1c4218-a5f9-418b-bf55-902b65277b85/'\n",
    "                 'quickstarts/connected-car-data/glove.6B.100d.txt')\n",
    "    urllib.request.urlretrieve(glove_url, 'glove.6B.100d.txt')\n",
    "    print(\"Download complete.\")\n",
    "\n",
    "download_glove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next load the data into a Pandas DataFrame and create the training, validation and test data sets by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Loading car components data...\n",
       "Loading car components data completed.\n",
       "Splitting data...\n",
       "(60000, 2)\n",
       "(20000, 2)\n",
       "(20000, 2)\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the car components labeled data\n",
    "print(\"Loading car components data...\")\n",
    "data_url = ('https://quickstartsws9073123377.blob.core.windows.net/'\n",
    "            'azureml-blobstore-0d1c4218-a5f9-418b-bf55-902b65277b85/'\n",
    "            'quickstarts/connected-car-data/connected-car_components.csv')\n",
    "car_components_df = pd.read_csv(data_url)\n",
    "components = car_components_df[\"text\"].tolist()\n",
    "labels = car_components_df[\"label\"].tolist()\n",
    "print(\"Loading car components data completed.\")\n",
    "\n",
    "# split data 60% for trianing, 20% for validation, 20% for test\n",
    "print(\"Splitting data...\")\n",
    "train, validate, test = np.split(car_components_df.sample(frac=1), [int(.6*len(car_components_df)), int(.8*len(car_components_df))])\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print(validate.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, you use the Tokenizer from Keras to \"learn\" a vocabulary from the entire car components text. Then the data (both the text and the compliance labels) is split into three subsets, one that will be used for training the deep learning model, one that will be used during training batches to tune the model weights and one that will be used after the model is trained to evaluate how it performs on data the model has never seen. \n",
    "\n",
    "Run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Tokenizing data...\n",
       "Found 65 unique tokens.\n",
       "Shape of data tensor: (100000, 15)\n",
       "Shape of label tensor: (100000,)\n",
       "Tokenizing data complete.\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use the Tokenizer from Keras to \"learn\" a vocabulary from the entire car components text\n",
    "print(\"Tokenizing data...\")\n",
    "\n",
    "maxlen = 15                                           \n",
    "training_samples = 90000                                 \n",
    "validation_samples = 5000    \n",
    "max_words = 10000      \n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(components)\n",
    "sequences = tokenizer.texts_to_sequences(components)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "labels = np.asarray(labels)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "indices = np.arange(data.shape[0])                     \n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "\n",
    "x_val = data[training_samples: training_samples + validation_samples]\n",
    "y_val = labels[training_samples: training_samples + validation_samples]\n",
    "\n",
    "x_test = data[training_samples + validation_samples:]\n",
    "y_test = labels[training_samples + validation_samples:]\n",
    "print(\"Tokenizing data complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now take a look at how the text was encoded as an array in the above. Run the following cell to take a peek.\n",
    "\n",
    "Each text vector will be of fixed length 100 since we defined maxlen to be 100 above. The following text: \"manufactured in 1971 made of carbon fiber in good condition\" has 10 words, and each word is represented by an integer value as encoded by the keras.preprocessing.text.Tokenizer. For example, the word \"manufactured\" is represented by the integer \"3\". Finally, the text vector is prepadded with zeros to fix the vector length to be 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">The text &apos;manufactured in 2003 made of steel in good condition&apos; is represented as the vector &apos;[ 0  0  0  0  0  0  3  1 19 10 11  9  1  8  2]&apos;\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"The text '{text}' is represented as the vector '{data}'\".format(text=components[indices[0]], data=x_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will apply the vectors provided by GloVe to create a word embedding matrix. This matrix will be used shortly to set the model wights of the first layer of the deep neural network. \n",
    "\n",
    "Run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Applying GloVe vectors...\n",
       "Found 400000 word vectors.\n",
       "Applying GloVe vectors completed.\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# apply the vectors provided by GloVe to create a word embedding matrix\n",
    "print(\"Applying GloVe vectors...\")\n",
    "glove_dir =  './'\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "embedding_dim = 100\n",
    "\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector    \n",
    "print(\"Applying GloVe vectors completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the LSTM recurrent neural network\n",
    "\n",
    "In the next cell, you will use Keras to define the structure of the deep neural network. In this case, we will build a LSTM recurrent neural network. The network will have a word embedding layer that will convert the word indices to GloVe word vectors. The GloVe word vectors are then passed to the LSTM layer, followed by an output layer.\n",
    "\n",
    "Run the following cell to structure the network and view a summary description of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Creating model structure...\n",
       "Model: &quot;sequential_1&quot;\n",
       "_________________________________________________________________\n",
       "Layer (type)                 Output Shape              Param #   \n",
       "=================================================================\n",
       "embedding_1 (Embedding)      (None, 15, 100)           1000000   \n",
       "_________________________________________________________________\n",
       "lstm_1 (LSTM)                (None, 100)               80400     \n",
       "_________________________________________________________________\n",
       "dense_1 (Dense)              (None, 1)                 101       \n",
       "=================================================================\n",
       "Total params: 1,080,501\n",
       "Trainable params: 80,501\n",
       "Non-trainable params: 1,000,000\n",
       "_________________________________________________________________\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use Keras to define the structure of the deep neural network   \n",
    "print(\"Creating model structure...\")\n",
    "\n",
    "embedding_layer = Embedding(embedding_matrix.shape[0],\n",
    "                            embedding_matrix.shape[1],\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=maxlen,\n",
    "                            trainable=False)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use MLflow with Azure Machine Learning for Model Training\n",
    "\n",
    "In the subsequent cells you will learn to do the following:\n",
    "- Set up MLflow tracking URI so as to use Azure ML\n",
    "- Create MLflow experiment â€“ this will create a corresponding experiment in Azure ML Workspace\n",
    "- Train a model on Azure Databricks cluster while logging metrics and artifacts using MLflow\n",
    "\n",
    "After this notebook, you should return to the `HOL step-by step - Machine Learning` guide and follow instructions to review the model performance metrics and training artifacts in the Azure Machine Learning workspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set MLflow tracking URI\n",
    "\n",
    "Set the MLflow tracking URI to point to your Azure ML Workspace. The subsequent logging calls from MLflow APIs will go to Azure ML services and will be tracked under your Workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">INFO: &apos;component-classifier&apos; does not exist. Creating a new experiment\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "experiment_name = \"component-classifier\"\n",
    "mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model and Log Metrics and Artifacts\n",
    "\n",
    "Now you are ready to train the model. Run the cell below to do the following:\n",
    "-\tLog model training metrics\n",
    "-\tTrain model\n",
    "-\tSave model\n",
    "-\tLog model training curves\n",
    "-\tEvaluate model\n",
    "-\tLog evaluation metrics\n",
    "\n",
    "Note that the metrics and artifacts will be recorded with your Azure ML Workspace.\n",
    "\n",
    "The cell will take **a few minutes** to run on a CPU cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Training model...\n",
       "Train on 90000 samples, validate on 5000 samples\n",
       "Epoch 1/5\n",
       "\n",
       "   16/90000 [..............................] - ETA: 56:08 - loss: 0.7443 - accuracy: 0.2500\n",
       "   96/90000 [..............................] - ETA: 10:19 - loss: 0.7258 - accuracy: 0.3958\n",
       "  176/90000 [..............................] - ETA: 6:09 - loss: 0.7122 - accuracy: 0.4489 \n",
       "  256/90000 [..............................] - ETA: 4:34 - loss: 0.7097 - accuracy: 0.4414\n",
       "  336/90000 [..............................] - ETA: 3:44 - loss: 0.7019 - accuracy: 0.4673\n",
       "  416/90000 [..............................] - ETA: 3:12 - loss: 0.6969 - accuracy: 0.5048\n",
       "  464/90000 [..............................] - ETA: 3:02 - loss: 0.6918 - accuracy: 0.5086\n",
       "  512/90000 [..............................] - ETA: 2:55 - loss: 0.6916 - accuracy: 0.5098\n",
       "  576/90000 [..............................] - ETA: 2:43 - loss: 0.6889 - accuracy: 0.5174\n",
       "  656/90000 [..............................] - ETA: 2:31 - loss: 0.6844 - accuracy: 0.5259\n",
       "  736/90000 [..............................] - ETA: 2:21 - loss: 0.6866 - accuracy: 0.5231\n",
       "  816/90000 [..............................] - ETA: 2:13 - loss: 0.6823 - accuracy: 0.5294\n",
       "  896/90000 [..............................] - ETA: 2:07 - loss: 0.6790 - accuracy: 0.5391\n",
       "  976/90000 [..............................] - ETA: 2:01 - loss: 0.6729 - accuracy: 0.5543\n",
       " 1056/90000 [..............................] - ETA: 1:56 - loss: 0.6695 - accuracy: 0.5616\n",
       " 1136/90000 [..............................] - ETA: 1:53 - loss: 0.6624 - accuracy: 0.5739\n",
       " 1216/90000 [..............................] - ETA: 1:49 - loss: 0.6532 - accuracy: 0.5888\n",
       " 1296/90000 [..............................] - ETA: 1:46 - loss: 0.6535 - accuracy: 0.5895\n",
       " 1376/90000 [..............................] - ETA: 1:44 - loss: 0.6442 - accuracy: 0.6061\n",
       " 1456/90000 [..............................] - ETA: 1:41 - loss: 0.6363 - accuracy: 0.6168\n",
       " 1536/90000 [..............................] - ETA: 1:39 - loss: 0.6295 - accuracy: 0.6217\n",
       " 1616/90000 [..............................] - ETA: 1:38 - loss: 0.6223 - accuracy: 0.6293\n",
       " 1696/90000 [..............................] - ETA: 1:36 - loss: 0.6132 - accuracy: 0.6380\n",
       " 1776/90000 [..............................] - ETA: 1:34 - loss: 0.6075 - accuracy: 0.6453\n",
       " 1856/90000 [..............................] - ETA: 1:32 - loss: 0.6053 - accuracy: 0.6498\n",
       " 1936/90000 [..............................] - ETA: 1:31 - loss: 0.6039 - accuracy: 0.6493\n",
       " 2016/90000 [..............................] - ETA: 1:30 - loss: 0.5982 - accuracy: 0.6548\n",
       " 2096/90000 [..............................] - ETA: 1:28 - loss: 0.5953 - accuracy: 0.6589\n",
       " 2176/90000 [..............................] - ETA: 1:27 - loss: 0.5893 - accuracy: 0.6654\n",
       " 2256/90000 [..............................] - ETA: 1:26 - loss: 0.5843 - accuracy: 0.6715\n",
       " 2336/90000 [..............................] - ETA: 1:25 - loss: 0.5791 - accuracy: 0.6785\n",
       " 2416/90000 [..............................] - ETA: 1:24 - loss: 0.5739 - accuracy: 0.6829\n",
       " 2496/90000 [..............................] - ETA: 1:23 - loss: 0.5653 - accuracy: 0.6903\n",
       " 2576/90000 [..............................] - ETA: 1:22 - loss: 0.5572 - accuracy: 0.6960\n",
       " 2656/90000 [..............................] - ETA: 1:22 - loss: 0.5499 - accuracy: 0.7014\n",
       " 2736/90000 [..............................] - ETA: 1:21 - loss: 0.5404 - accuracy: 0.7083\n",
       " 2816/90000 [..............................] - ETA: 1:20 - loss: 0.5343 - accuracy: 0.7127\n",
       " 2896/90000 [..............................] - ETA: 1:19 - loss: 0.5272 - accuracy: 0.7186\n",
       " 2976/90000 [..............................] - ETA: 1:19 - loss: 0.5211 - accuracy: 0.7228\n",
       " 3056/90000 [&gt;.............................] - ETA: 1:18 - loss: 0.5148 - accuracy: 0.7268\n",
       " 3136/90000 [&gt;.............................] - ETA: 1:18 - loss: 0.5081 - accuracy: 0.7315\n",
       " 3216/90000 [&gt;.............................] - ETA: 1:17 - loss: 0.5030 - accuracy: 0.7345\n",
       " 3296/90000 [&gt;.............................] - ETA: 1:17 - loss: 0.4961 - accuracy: 0.7385\n",
       " 3376/90000 [&gt;.............................] - ETA: 1:16 - loss: 0.4910 - accuracy: 0.7414\n",
       " 3456/90000 [&gt;.............................] - ETA: 1:16 - loss: 0.4860 - accuracy: 0.7451\n",
       " 3536/90000 [&gt;.............................] - ETA: 1:15 - loss: 0.4855 - accuracy: 0.7452\n",
       " 3616/90000 [&gt;.............................] - ETA: 1:15 - loss: 0.4800 - accuracy: 0.7492\n",
       " 3696/90000 [&gt;.............................] - ETA: 1:15 - loss: 0.4777 - accuracy: 0.7519\n",
       " 3776/90000 [&gt;.............................] - ETA: 1:14 - loss: 0.4727 - accuracy: 0.7550\n",
       " 3856/90000 [&gt;.............................] - ETA: 1:14 - loss: 0.4672 - accuracy: 0.7575\n",
       " 3936/90000 [&gt;.............................] - ETA: 1:13 - loss: 0.4610 - accuracy: 0.7619\n",
       " 4016/90000 [&gt;.............................] - ETA: 1:13 - loss: 0.4541 - accuracy: 0.7664\n",
       " 4096/90000 [&gt;.............................] - ETA: 1:13 - loss: 0.4477 - accuracy: 0.7705\n",
       " 4176/90000 [&gt;.............................] - ETA: 1:12 - loss: 0.4415 - accuracy: 0.7742\n",
       " 4256/90000 [&gt;.............................] - ETA: 1:12 - loss: 0.4365 - accuracy: 0.7770\n",
       " 4336/90000 [&gt;.............................] - ETA: 1:12 - loss: 0.4308 - accuracy: 0.7802\n",
       " 4416/90000 [&gt;.............................] - ETA: 1:12 - loss: 0.4257 - accuracy: 0.7831\n",
       " 4496/90000 [&gt;.............................] - ETA: 1:11 - loss: 0.4210 - accuracy: 0.7865\n",
       " 4576/90000 [&gt;.............................] - ETA: 1:11 - loss: 0.4172 - accuracy: 0.7891\n",
       " 4656/90000 [&gt;.............................] - ETA: 1:11 - loss: 0.4111 - accuracy: 0.7927\n",
       " 4736/90000 [&gt;.............................] - ETA: 1:10 - loss: 0.4057 - accuracy: 0.7958\n",
       " 4816/90000 [&gt;.............................] - ETA: 1:10 - loss: 0.4007 - accuracy: 0.7986\n",
       " 4896/90000 [&gt;.............................] - ETA: 1:10 - loss: 0.3950 - accuracy: 0.8015\n",
       " 4976/90000 [&gt;.............................] - ETA: 1:10 - loss: 0.3904 - accuracy: 0.8039\n",
       " 5056/90000 [&gt;.............................] - ETA: 1:09 - loss: 0.3852 - accuracy: 0.8068\n",
       " 5136/90000 [&gt;.............................] - ETA: 1:09 - loss: 0.3803 - accuracy: 0.8096\n",
       " 5216/90000 [&gt;.............................] - ETA: 1:09 - loss: 0.3759 - accuracy: 0.8119\n",
       " 5296/90000 [&gt;.............................] - ETA: 1:09 - loss: 0.3712 - accuracy: 0.8146\n",
       " 5376/90000 [&gt;.............................] - ETA: 1:08 - loss: 0.3669 - accuracy: 0.8168\n",
       " 5456/90000 [&gt;.............................] - ETA: 1:08 - loss: 0.3637 - accuracy: 0.8189\n",
       " 5536/90000 [&gt;.............................] - ETA: 1:08 - loss: 0.3602 - accuracy: 0.8210\n",
       " 5616/90000 [&gt;.............................] - ETA: 1:07 - loss: 0.3570 - accuracy: 0.8228\n",
       " 5696/90000 [&gt;.............................] - ETA: 1:07 - loss: 0.3541 - accuracy: 0.8248\n",
       " 5776/90000 [&gt;.............................] - ETA: 1:07 - loss: 0.3509 - accuracy: 0.8265\n",
       " 5856/90000 [&gt;.............................] - ETA: 1:07 - loss: 0.3473 - accuracy: 0.8286\n",
       " 5936/90000 [&gt;.............................] - ETA: 1:07 - loss: 0.3435 - accuracy: 0.8307\n",
       " 6016/90000 [=&gt;............................] - ETA: 1:06 - loss: 0.3397 - accuracy: 0.8328\n",
       " 6096/90000 [=&gt;............................] - ETA: 1:06 - loss: 0.3357 - accuracy: 0.8350\n",
       " 6176/90000 [=&gt;............................] - ETA: 1:06 - loss: 0.3316 - accuracy: 0.8371\n",
       " 6256/90000 [=&gt;............................] - ETA: 1:06 - loss: 0.3279 - accuracy: 0.8392\n",
       " 6336/90000 [=&gt;............................] - ETA: 1:06 - loss: 0.3245 - accuracy: 0.8409\n",
       " 6416/90000 [=&gt;............................] - ETA: 1:05 - loss: 0.3222 - accuracy: 0.8424\n",
       " 6496/90000 [=&gt;............................] - ETA: 1:05 - loss: 0.3188 - accuracy: 0.8441\n",
       " 6576/90000 [=&gt;............................] - ETA: 1:05 - loss: 0.3157 - accuracy: 0.8458\n",
       " 6656/90000 [=&gt;............................] - ETA: 1:05 - loss: 0.3122 - accuracy: 0.8477\n",
       " 6736/90000 [=&gt;............................] - ETA: 1:05 - loss: 0.3092 - accuracy: 0.8493\n",
       " 6816/90000 [=&gt;............................] - ETA: 1:05 - loss: 0.3070 - accuracy: 0.8505\n",
       " 6896/90000 [=&gt;............................] - ETA: 1:05 - loss: 0.3037 - accuracy: 0.8522\n",
       " 6976/90000 [=&gt;............................] - ETA: 1:04 - loss: 0.3011 - accuracy: 0.8538\n",
       " 7056/90000 [=&gt;............................] - ETA: 1:04 - loss: 0.2979 - accuracy: 0.8554\n",
       " 7136/90000 [=&gt;............................] - ETA: 1:04 - loss: 0.2949 - accuracy: 0.8571\n",
       " 7216/90000 [=&gt;............................] - ETA: 1:04 - loss: 0.2921 - accuracy: 0.8585\n",
       " 7296/90000 [=&gt;............................] - ETA: 1:04 - loss: 0.2891 - accuracy: 0.8601\n",
       " 7376/90000 [=&gt;............................] - ETA: 1:04 - loss: 0.2862 - accuracy: 0.8616\n",
       " 7456/90000 [=&gt;............................] - ETA: 1:04 - loss: 0.2836 - accuracy: 0.8631\n",
       " 7536/90000 [=&gt;............................] - ETA: 1:04 - loss: 0.2809 - accuracy: 0.8644\n",
       " 7616/90000 [=&gt;............................] - ETA: 1:03 - loss: 0.2781 - accuracy: 0.8658\n",
       " 7696/90000 [=&gt;............................] - ETA: 1:03 - loss: 0.2758 - accuracy: 0.8669\n",
       " 7776/90000 [=&gt;............................] - ETA: 1:03 - loss: 0.2731 - accuracy: 0.8683\n",
       " 7856/90000 [=&gt;............................] - ETA: 1:03 - loss: 0.2718 - accuracy: 0.8693\n",
       " 7936/90000 [=&gt;............................] - ETA: 1:03 - loss: 0.2692 - accuracy: 0.8706\n",
       " 8016/90000 [=&gt;............................] - ETA: 1:03 - loss: 0.2678 - accuracy: 0.8715\n",
       " 8096/90000 [=&gt;............................] - ETA: 1:03 - loss: 0.2655 - accuracy: 0.8728\n",
       " 8176/90000 [=&gt;............................] - ETA: 1:02 - loss: 0.2632 - accuracy: 0.8739\n",
       " 8256/90000 [=&gt;............................] - ETA: 1:02 - loss: 0.2609 - accuracy: 0.8751\n",
       " 8336/90000 [=&gt;............................] - ETA: 1:02 - loss: 0.2588 - accuracy: 0.8762\n",
       " 8416/90000 [=&gt;............................] - ETA: 1:02 - loss: 0.2567 - accuracy: 0.8771\n",
       " 8496/90000 [=&gt;............................] - ETA: 1:02 - loss: 0.2546 - accuracy: 0.8782\n",
       " 8576/90000 [=&gt;............................] - ETA: 1:02 - loss: 0.2531 - accuracy: 0.8792\n",
       " 8656/90000 [=&gt;............................] - ETA: 1:02 - loss: 0.2511 - accuracy: 0.8802\n",
       " 8736/90000 [=&gt;............................] - ETA: 1:01 - loss: 0.2489 - accuracy: 0.8813\n",
       " 8816/90000 [=&gt;............................] - ETA: 1:01 - loss: 0.2468 - accuracy: 0.8824\n",
       " 8896/90000 [=&gt;............................] - ETA: 1:01 - loss: 0.2451 - accuracy: 0.8833\n",
       " 8976/90000 [=&gt;............................] - ETA: 1:01 - loss: 0.2436 - accuracy: 0.8841\n",
       " 9056/90000 [==&gt;...........................] - ETA: 1:01 - loss: 0.2417 - accuracy: 0.8850\n",
       " 9136/90000 [==&gt;...........................] - ETA: 1:01 - loss: 0.2397 - accuracy: 0.8861\n",
       " 9216/90000 [==&gt;...........................] - ETA: 1:01 - loss: 0.2377 - accuracy: 0.8870\n",
       " 9296/90000 [==&gt;...........................] - ETA: 1:01 - loss: 0.2361 - accuracy: 0.8880\n",
       " 9376/90000 [==&gt;...........................] - ETA: 1:01 - loss: 0.2342 - accuracy: 0.8889\n",
       " 9456/90000 [==&gt;...........................] - ETA: 1:01 - loss: 0.2326 - accuracy: 0.8896\n",
       " 9536/90000 [==&gt;...........................] - ETA: 1:00 - loss: 0.2309 - accuracy: 0.8904\n",
       " 9616/90000 [==&gt;...........................] - ETA: 1:00 - loss: 0.2292 - accuracy: 0.8912\n",
       " 9696/90000 [==&gt;...........................] - ETA: 1:00 - loss: 0.2274 - accuracy: 0.8921\n",
       " 9776/90000 [==&gt;...........................] - ETA: 1:00 - loss: 0.2257 - accuracy: 0.8930\n",
       " 9856/90000 [==&gt;...........................] - ETA: 1:00 - loss: 0.2243 - accuracy: 0.8938\n",
       " 9936/90000 [==&gt;...........................] - ETA: 1:00 - loss: 0.2228 - accuracy: 0.8945\n",
       "10016/90000 [==&gt;...........................] - ETA: 1:00 - loss: 0.2211 - accuracy: 0.8954\n",
       "10096/90000 [==&gt;...........................] - ETA: 1:00 - loss: 0.2195 - accuracy: 0.8962\n",
       "10176/90000 [==&gt;...........................] - ETA: 1:00 - loss: 0.2179 - accuracy: 0.8970\n",
       "10256/90000 [==&gt;...........................] - ETA: 59s - loss: 0.2163 - accuracy: 0.8978 \n",
       "10336/90000 [==&gt;...........................] - ETA: 59s - loss: 0.2149 - accuracy: 0.8984\n",
       "10416/90000 [==&gt;...........................] - ETA: 59s - loss: 0.2135 - accuracy: 0.8991\n",
       "10496/90000 [==&gt;...........................] - ETA: 59s - loss: 0.2120 - accuracy: 0.8999\n",
       "10576/90000 [==&gt;...........................] - ETA: 59s - loss: 0.2104 - accuracy: 0.9006\n",
       "10656/90000 [==&gt;...........................] - ETA: 59s - loss: 0.2090 - accuracy: 0.9014\n",
       "10736/90000 [==&gt;...........................] - ETA: 59s - loss: 0.2076 - accuracy: 0.9021\n",
       "10800/90000 [==&gt;...........................] - ETA: 59s - loss: 0.2065 - accuracy: 0.9026\n",
       "10880/90000 [==&gt;...........................] - ETA: 59s - loss: 0.2051 - accuracy: 0.9033\n",
       "10960/90000 [==&gt;...........................] - ETA: 59s - loss: 0.2037 - accuracy: 0.9040\n",
       "11040/90000 [==&gt;...........................] - ETA: 59s - loss: 0.2023 - accuracy: 0.9047\n",
       "11120/90000 [==&gt;...........................] - ETA: 58s - loss: 0.2008 - accuracy: 0.9054\n",
       "11200/90000 [==&gt;...........................] - ETA: 58s - loss: 0.1996 - accuracy: 0.9060\n",
       "11280/90000 [==&gt;...........................] - ETA: 58s - loss: 0.1982 - accuracy: 0.9066\n",
       "11360/90000 [==&gt;...........................] - ETA: 58s - loss: 0.1969 - accuracy: 0.9073\n",
       "11440/90000 [==&gt;...........................] - ETA: 58s - loss: 0.1956 - accuracy: 0.9080\n",
       "11520/90000 [==&gt;...........................] - ETA: 58s - loss: 0.1944 - accuracy: 0.9085\n",
       "11600/90000 [==&gt;...........................] - ETA: 58s - loss: 0.1931 - accuracy: 0.9091\n",
       "11680/90000 [==&gt;...........................] - ETA: 58s - loss: 0.1920 - accuracy: 0.9097\n",
       "11760/90000 [==&gt;...........................] - ETA: 58s - loss: 0.1908 - accuracy: 0.9103\n",
       "11840/90000 [==&gt;...........................] - ETA: 57s - loss: 0.1895 - accuracy: 0.9109\n",
       "11920/90000 [==&gt;...........................] - ETA: 57s - loss: 0.1883 - accuracy: 0.9115\n",
       "12000/90000 [===&gt;..........................] - ETA: 57s - loss: 0.1872 - accuracy: 0.9121\n",
       "12080/90000 [===&gt;..........................] - ETA: 57s - loss: 0.1860 - accuracy: 0.9127\n",
       "12160/90000 [===&gt;..........................] - ETA: 57s - loss: 0.1848 - accuracy: 0.9132\n",
       "12240/90000 [===&gt;..........................] - ETA: 57s - loss: 0.1841 - accuracy: 0.9136\n",
       "12320/90000 [===&gt;..........................] - ETA: 57s - loss: 0.1831 - accuracy: 0.9141\n",
       "12400/90000 [===&gt;..........................] - ETA: 57s - loss: 0.1831 - accuracy: 0.9144\n",
       "12480/90000 [===&gt;..........................] - ETA: 57s - loss: 0.1826 - accuracy: 0.9148\n",
       "12560/90000 [===&gt;..........................] - ETA: 57s - loss: 0.1817 - accuracy: 0.9153\n",
       "12640/90000 [===&gt;..........................] - ETA: 57s - loss: 0.1809 - accuracy: 0.9157\n",
       "12720/90000 [===&gt;..........................] - ETA: 56s - loss: 0.1800 - accuracy: 0.9161\n",
       "12800/90000 [===&gt;..........................] - ETA: 56s - loss: 0.1792 - accuracy: 0.9166\n",
       "12880/90000 [===&gt;..........................] - ETA: 56s - loss: 0.1782 - accuracy: 0.9170\n",
       "12960/90000 [===&gt;..........................] - ETA: 56s - loss: 0.1771 - accuracy: 0.9175\n",
       "13040/90000 [===&gt;..........................] - ETA: 56s - loss: 0.1762 - accuracy: 0.9180\n",
       "13120/90000 [===&gt;..........................] - ETA: 56s - loss: 0.1752 - accuracy: 0.9185\n",
       "13200/90000 [===&gt;..........................] - ETA: 56s - loss: 0.1742 - accuracy: 0.9190\n",
       "13280/90000 [===&gt;..........................] - ETA: 56s - loss: 0.1732 - accuracy: 0.9195\n",
       "13360/90000 [===&gt;..........................] - ETA: 56s - loss: 0.1722 - accuracy: 0.9200\n",
       "13440/90000 [===&gt;..........................] - ETA: 56s - loss: 0.1712 - accuracy: 0.9205\n",
       "13520/90000 [===&gt;..........................] - ETA: 56s - loss: 0.1702 - accuracy: 0.9209\n",
       "13600/90000 [===&gt;..........................] - ETA: 56s - loss: 0.1693 - accuracy: 0.9213\n",
       "13680/90000 [===&gt;..........................] - ETA: 55s - loss: 0.1686 - accuracy: 0.9217\n",
       "13760/90000 [===&gt;..........................] - ETA: 55s - loss: 0.1677 - accuracy: 0.9222\n",
       "13840/90000 [===&gt;..........................] - ETA: 55s - loss: 0.1668 - accuracy: 0.9226\n",
       "13920/90000 [===&gt;..........................] - ETA: 55s - loss: 0.1658 - accuracy: 0.9231\n",
       "14000/90000 [===&gt;..........................] - ETA: 55s - loss: 0.1649 - accuracy: 0.9235\n",
       "14064/90000 [===&gt;..........................] - ETA: 55s - loss: 0.1642 - accuracy: 0.9238\n",
       "14144/90000 [===&gt;..........................] - ETA: 55s - loss: 0.1633 - accuracy: 0.9243\n",
       "14224/90000 [===&gt;..........................] - ETA: 55s - loss: 0.1624 - accuracy: 0.9247\n",
       "14304/90000 [===&gt;..........................] - ETA: 55s - loss: 0.1615 - accuracy: 0.9251\n",
       "14384/90000 [===&gt;..........................] - ETA: 55s - loss: 0.1607 - accuracy: 0.9255\n",
       "14464/90000 [===&gt;..........................] - ETA: 55s - loss: 0.1599 - accuracy: 0.9259\n",
       "14544/90000 [===&gt;..........................] - ETA: 55s - loss: 0.1591 - accuracy: 0.9262\n",
       "14624/90000 [===&gt;..........................] - ETA: 55s - loss: 0.1583 - accuracy: 0.9266\n",
       "14704/90000 [===&gt;..........................] - ETA: 55s - loss: 0.1575 - accuracy: 0.9270\n",
       "14784/90000 [===&gt;..........................] - ETA: 54s - loss: 0.1567 - accuracy: 0.9274\n",
       "14864/90000 [===&gt;..........................] - ETA: 54s - loss: 0.1559 - accuracy: 0.9278\n",
       "14944/90000 [===&gt;..........................] - ETA: 54s - loss: 0.1551 - accuracy: 0.9282\n",
       "15024/90000 [====&gt;.........................] - ETA: 54s - loss: 0.1543 - accuracy: 0.9286\n",
       "15104/90000 [====&gt;.........................] - ETA: 54s - loss: 0.1535 - accuracy: 0.9290\n",
       "15184/90000 [====&gt;.........................] - ETA: 54s - loss: 0.1527 - accuracy: 0.9293\n",
       "15280/90000 [====&gt;.........................] - ETA: 54s - loss: 0.1518 - accuracy: 0.9298\n",
       "15360/90000 [====&gt;.........................] - ETA: 54s - loss: 0.1510 - accuracy: 0.9301\n",
       "15440/90000 [====&gt;.........................] - ETA: 54s - loss: 0.1503 - accuracy: 0.9305\n",
       "15520/90000 [====&gt;.........................] - ETA: 54s - loss: 0.1497 - accuracy: 0.9308\n",
       "15600/90000 [====&gt;.........................] - ETA: 54s - loss: 0.1491 - accuracy: 0.9311\n",
       "15680/90000 [====&gt;.........................] - ETA: 54s - loss: 0.1484 - accuracy: 0.9314\n",
       "15760/90000 [====&gt;.........................] - ETA: 53s - loss: 0.1477 - accuracy: 0.9318\n",
       "15840/90000 [====&gt;.........................] - ETA: 53s - loss: 0.1471 - accuracy: 0.9321\n",
       "15920/90000 [====&gt;.........................] - ETA: 53s - loss: 0.1464 - accuracy: 0.9324\n",
       "16000/90000 [====&gt;.........................] - ETA: 53s - loss: 0.1457 - accuracy: 0.9327\n",
       "16080/90000 [====&gt;.........................] - ETA: 53s - loss: 0.1451 - accuracy: 0.9330\n",
       "16160/90000 [====&gt;.........................] - ETA: 53s - loss: 0.1444 - accuracy: 0.9334\n",
       "16240/90000 [====&gt;.........................] - ETA: 53s - loss: 0.1438 - accuracy: 0.9336\n",
       "16320/90000 [====&gt;.........................] - ETA: 53s - loss: 0.1432 - accuracy: 0.9339\n",
       "16384/90000 [====&gt;.........................] - ETA: 53s - loss: 0.1426 - accuracy: 0.9342\n",
       "16464/90000 [====&gt;.........................] - ETA: 53s - loss: 0.1420 - accuracy: 0.9345\n",
       "16544/90000 [====&gt;.........................] - ETA: 53s - loss: 0.1413 - accuracy: 0.9348\n",
       "16624/90000 [====&gt;.........................] - ETA: 53s - loss: 0.1408 - accuracy: 0.9351\n",
       "16704/90000 [====&gt;.........................] - ETA: 53s - loss: 0.1401 - accuracy: 0.9354\n",
       "16784/90000 [====&gt;.........................] - ETA: 53s - loss: 0.1395 - accuracy: 0.9357\n",
       "16864/90000 [====&gt;.........................] - ETA: 52s - loss: 0.1389 - accuracy: 0.9360\n",
       "16944/90000 [====&gt;.........................] - ETA: 52s - loss: 0.1385 - accuracy: 0.9362\n",
       "17024/90000 [====&gt;.........................] - ETA: 52s - loss: 0.1379 - accuracy: 0.9365\n",
       "17104/90000 [====&gt;.........................] - ETA: 52s - loss: 0.1373 - accuracy: 0.9368\n",
       "17184/90000 [====&gt;.........................] - ETA: 52s - loss: 0.1367 - accuracy: 0.9371\n",
       "17264/90000 [====&gt;.........................] - ETA: 52s - loss: 0.1361 - accuracy: 0.9374\n",
       "17344/90000 [====&gt;.........................] - ETA: 52s - loss: 0.1355 - accuracy: 0.9377\n",
       "17424/90000 [====&gt;.........................] - ETA: 52s - loss: 0.1350 - accuracy: 0.9380\n",
       "17504/90000 [====&gt;.........................] - ETA: 52s - loss: 0.1344 - accuracy: 0.9382\n",
       "17584/90000 [====&gt;.........................] - ETA: 52s - loss: 0.1338 - accuracy: 0.9385\n",
       "17648/90000 [====&gt;.........................] - ETA: 52s - loss: 0.1337 - accuracy: 0.9386\n",
       "17728/90000 [====&gt;.........................] - ETA: 52s - loss: 0.1331 - accuracy: 0.9389\n",
       "17808/90000 [====&gt;.........................] - ETA: 52s - loss: 0.1326 - accuracy: 0.9391\n",
       "17888/90000 [====&gt;.........................] - ETA: 52s - loss: 0.1320 - accuracy: 0.9394\n",
       "17968/90000 [====&gt;.........................] - ETA: 52s - loss: 0.1317 - accuracy: 0.9396\n",
       "18048/90000 [=====&gt;........................] - ETA: 52s - loss: 0.1311 - accuracy: 0.9398\n",
       "18128/90000 [=====&gt;........................] - ETA: 51s - loss: 0.1306 - accuracy: 0.9400\n",
       "18208/90000 [=====&gt;........................] - ETA: 51s - loss: 0.1303 - accuracy: 0.9402\n",
       "18288/90000 [=====&gt;........................] - ETA: 51s - loss: 0.1299 - accuracy: 0.9404\n",
       "18368/90000 [=====&gt;........................] - ETA: 51s - loss: 0.1294 - accuracy: 0.9406\n",
       "18448/90000 [=====&gt;........................] - ETA: 51s - loss: 0.1289 - accuracy: 0.9409\n",
       "18528/90000 [=====&gt;........................] - ETA: 51s - loss: 0.1283 - accuracy: 0.9411\n",
       "18608/90000 [=====&gt;........................] - ETA: 51s - loss: 0.1278 - accuracy: 0.9414\n",
       "18688/90000 [=====&gt;........................] - ETA: 51s - loss: 0.1273 - accuracy: 0.9416\n",
       "18768/90000 [=====&gt;........................] - ETA: 51s - loss: 0.1268 - accuracy: 0.9419\n",
       "18848/90000 [=====&gt;........................] - ETA: 51s - loss: 0.1263 - accuracy: 0.9421\n",
       "18928/90000 [=====&gt;........................] - ETA: 51s - loss: 0.1258 - accuracy: 0.9423\n",
       "19008/90000 [=====&gt;........................] - ETA: 51s - loss: 0.1253 - accuracy: 0.9426\n",
       "19088/90000 [=====&gt;........................] - ETA: 51s - loss: 0.1248 - accuracy: 0.9428\n",
       "19184/90000 [=====&gt;........................] - ETA: 50s - loss: 0.1243 - accuracy: 0.9431\n",
       "19264/90000 [=====&gt;........................] - ETA: 50s - loss: 0.1238 - accuracy: 0.9433\n",
       "19344/90000 [=====&gt;........................] - ETA: 50s - loss: 0.1233 - accuracy: 0.9435\n",
       "19424/90000 [=====&gt;........................] - ETA: 50s - loss: 0.1228 - accuracy: 0.9438\n",
       "19504/90000 [=====&gt;........................] - ETA: 50s - loss: 0.1223 - accuracy: 0.9440\n",
       "19584/90000 [=====&gt;........................] - ETA: 50s - loss: 0.1219 - accuracy: 0.9442\n",
       "19664/90000 [=====&gt;........................] - ETA: 50s - loss: 0.1214 - accuracy: 0.9445\n",
       "19744/90000 [=====&gt;........................] - ETA: 50s - loss: 0.1209 - accuracy: 0.9447\n",
       "19824/90000 [=====&gt;........................] - ETA: 50s - loss: 0.1204 - accuracy: 0.9449\n",
       "19904/90000 [=====&gt;........................] - ETA: 50s - loss: 0.1200 - accuracy: 0.9451\n",
       "19984/90000 [=====&gt;........................] - ETA: 50s - loss: 0.1195 - accuracy: 0.9454\n",
       "20064/90000 [=====&gt;........................] - ETA: 50s - loss: 0.1190 - accuracy: 0.9456\n",
       "20144/90000 [=====&gt;........................] - ETA: 50s - loss: 0.1187 - accuracy: 0.9457\n",
       "20224/90000 [=====&gt;........................] - ETA: 50s - loss: 0.1182 - accuracy: 0.9460\n",
       "20304/90000 [=====&gt;........................] - ETA: 49s - loss: 0.1178 - accuracy: 0.9461\n",
       "20384/90000 [=====&gt;........................] - ETA: 49s - loss: 0.1174 - accuracy: 0.9463\n",
       "20464/90000 [=====&gt;........................] - ETA: 49s - loss: 0.1169 - accuracy: 0.9465\n",
       "20544/90000 [=====&gt;........................] - ETA: 49s - loss: 0.1165 - accuracy: 0.9467\n",
       "20624/90000 [=====&gt;........................] - ETA: 49s - loss: 0.1162 - accuracy: 0.9469\n",
       "20720/90000 [=====&gt;........................] - ETA: 49s - loss: 0.1156 - accuracy: 0.9471\n",
       "20816/90000 [=====&gt;........................] - ETA: 49s - loss: 0.1154 - accuracy: 0.9472\n",
       "20896/90000 [=====&gt;........................] - ETA: 49s - loss: 0.1150 - accuracy: 0.9474\n",
       "20960/90000 [=====&gt;........................] - ETA: 49s - loss: 0.1146 - accuracy: 0.9476\n",
       "21040/90000 [======&gt;.......................] - ETA: 49s - loss: 0.1142 - accuracy: 0.9478\n",
       "21120/90000 [======&gt;.......................] - ETA: 49s - loss: 0.1139 - accuracy: 0.9479\n",
       "21200/90000 [======&gt;.......................] - ETA: 49s - loss: 0.1135 - accuracy: 0.9481\n",
       "21280/90000 [======&gt;.......................] - ETA: 49s - loss: 0.1131 - accuracy: 0.9483\n",
       "21360/90000 [======&gt;.......................] - ETA: 49s - loss: 0.1127 - accuracy: 0.9485\n",
       "21440/90000 [======&gt;.......................] - ETA: 48s - loss: 0.1123 - accuracy: 0.9487\n",
       "21520/90000 [======&gt;.......................] - ETA: 48s - loss: 0.1119 - accuracy: 0.9489\n",
       "21600/90000 [======&gt;.......................] - ETA: 48s - loss: 0.1115 - accuracy: 0.9491\n",
       "21680/90000 [======&gt;.......................] - ETA: 48s - loss: 0.1111 - accuracy: 0.9493\n",
       "21760/90000 [======&gt;.......................] - ETA: 48s - loss: 0.1107 - accuracy: 0.9494\n",
       "21856/90000 [======&gt;.......................] - ETA: 48s - loss: 0.1103 - accuracy: 0.9497\n",
       "21936/90000 [======&gt;.......................] - ETA: 48s - loss: 0.1099 - accuracy: 0.9498\n",
       "\n",
       "*** WARNING: skipped 470720 bytes of output ***\n",
       "\n",
       "69584/90000 [======================&gt;.......] - ETA: 14s - loss: 1.1085e-04 - accuracy: 1.0000\n",
       "69664/90000 [======================&gt;.......] - ETA: 14s - loss: 1.1072e-04 - accuracy: 1.0000\n",
       "69744/90000 [======================&gt;.......] - ETA: 13s - loss: 1.1060e-04 - accuracy: 1.0000\n",
       "69824/90000 [======================&gt;.......] - ETA: 13s - loss: 1.1049e-04 - accuracy: 1.0000\n",
       "69904/90000 [======================&gt;.......] - ETA: 13s - loss: 1.1036e-04 - accuracy: 1.0000\n",
       "69984/90000 [======================&gt;.......] - ETA: 13s - loss: 1.1024e-04 - accuracy: 1.0000\n",
       "70064/90000 [======================&gt;.......] - ETA: 13s - loss: 1.1012e-04 - accuracy: 1.0000\n",
       "70144/90000 [======================&gt;.......] - ETA: 13s - loss: 1.1000e-04 - accuracy: 1.0000\n",
       "70224/90000 [======================&gt;.......] - ETA: 13s - loss: 1.0988e-04 - accuracy: 1.0000\n",
       "70304/90000 [======================&gt;.......] - ETA: 13s - loss: 1.0976e-04 - accuracy: 1.0000\n",
       "70384/90000 [======================&gt;.......] - ETA: 13s - loss: 1.0964e-04 - accuracy: 1.0000\n",
       "70464/90000 [======================&gt;.......] - ETA: 13s - loss: 1.0951e-04 - accuracy: 1.0000\n",
       "70544/90000 [======================&gt;.......] - ETA: 13s - loss: 1.0939e-04 - accuracy: 1.0000\n",
       "70624/90000 [======================&gt;.......] - ETA: 13s - loss: 1.0927e-04 - accuracy: 1.0000\n",
       "70704/90000 [======================&gt;.......] - ETA: 13s - loss: 1.0916e-04 - accuracy: 1.0000\n",
       "70784/90000 [======================&gt;.......] - ETA: 13s - loss: 1.0904e-04 - accuracy: 1.0000\n",
       "70864/90000 [======================&gt;.......] - ETA: 13s - loss: 1.0892e-04 - accuracy: 1.0000\n",
       "70944/90000 [======================&gt;.......] - ETA: 13s - loss: 1.0884e-04 - accuracy: 1.0000\n",
       "71024/90000 [======================&gt;.......] - ETA: 13s - loss: 1.0872e-04 - accuracy: 1.0000\n",
       "71104/90000 [======================&gt;.......] - ETA: 13s - loss: 1.0883e-04 - accuracy: 1.0000\n",
       "71184/90000 [======================&gt;.......] - ETA: 12s - loss: 1.0871e-04 - accuracy: 1.0000\n",
       "71264/90000 [======================&gt;.......] - ETA: 12s - loss: 1.0859e-04 - accuracy: 1.0000\n",
       "71344/90000 [======================&gt;.......] - ETA: 12s - loss: 1.0848e-04 - accuracy: 1.0000\n",
       "71424/90000 [======================&gt;.......] - ETA: 12s - loss: 1.0836e-04 - accuracy: 1.0000\n",
       "71504/90000 [======================&gt;.......] - ETA: 12s - loss: 1.0824e-04 - accuracy: 1.0000\n",
       "71584/90000 [======================&gt;.......] - ETA: 12s - loss: 1.0812e-04 - accuracy: 1.0000\n",
       "71664/90000 [======================&gt;.......] - ETA: 12s - loss: 1.0804e-04 - accuracy: 1.0000\n",
       "71744/90000 [======================&gt;.......] - ETA: 12s - loss: 1.0793e-04 - accuracy: 1.0000\n",
       "71824/90000 [======================&gt;.......] - ETA: 12s - loss: 1.0781e-04 - accuracy: 1.0000\n",
       "71904/90000 [======================&gt;.......] - ETA: 12s - loss: 1.0769e-04 - accuracy: 1.0000\n",
       "71984/90000 [======================&gt;.......] - ETA: 12s - loss: 1.0758e-04 - accuracy: 1.0000\n",
       "72064/90000 [=======================&gt;......] - ETA: 12s - loss: 1.0752e-04 - accuracy: 1.0000\n",
       "72144/90000 [=======================&gt;......] - ETA: 12s - loss: 1.0740e-04 - accuracy: 1.0000\n",
       "72224/90000 [=======================&gt;......] - ETA: 12s - loss: 1.0728e-04 - accuracy: 1.0000\n",
       "72304/90000 [=======================&gt;......] - ETA: 12s - loss: 1.0716e-04 - accuracy: 1.0000\n",
       "72384/90000 [=======================&gt;......] - ETA: 12s - loss: 1.0705e-04 - accuracy: 1.0000\n",
       "72448/90000 [=======================&gt;......] - ETA: 12s - loss: 1.0695e-04 - accuracy: 1.0000\n",
       "72528/90000 [=======================&gt;......] - ETA: 12s - loss: 1.0684e-04 - accuracy: 1.0000\n",
       "72608/90000 [=======================&gt;......] - ETA: 11s - loss: 1.0672e-04 - accuracy: 1.0000\n",
       "72688/90000 [=======================&gt;......] - ETA: 11s - loss: 1.0661e-04 - accuracy: 1.0000\n",
       "72768/90000 [=======================&gt;......] - ETA: 11s - loss: 1.0652e-04 - accuracy: 1.0000\n",
       "72848/90000 [=======================&gt;......] - ETA: 11s - loss: 1.0643e-04 - accuracy: 1.0000\n",
       "72928/90000 [=======================&gt;......] - ETA: 11s - loss: 1.0631e-04 - accuracy: 1.0000\n",
       "73008/90000 [=======================&gt;......] - ETA: 11s - loss: 1.0620e-04 - accuracy: 1.0000\n",
       "73088/90000 [=======================&gt;......] - ETA: 11s - loss: 1.0608e-04 - accuracy: 1.0000\n",
       "73168/90000 [=======================&gt;......] - ETA: 11s - loss: 1.0597e-04 - accuracy: 1.0000\n",
       "73248/90000 [=======================&gt;......] - ETA: 11s - loss: 1.0731e-04 - accuracy: 1.0000\n",
       "73328/90000 [=======================&gt;......] - ETA: 11s - loss: 1.0720e-04 - accuracy: 1.0000\n",
       "73408/90000 [=======================&gt;......] - ETA: 11s - loss: 1.0709e-04 - accuracy: 1.0000\n",
       "73488/90000 [=======================&gt;......] - ETA: 11s - loss: 1.0697e-04 - accuracy: 1.0000\n",
       "73568/90000 [=======================&gt;......] - ETA: 11s - loss: 1.0686e-04 - accuracy: 1.0000\n",
       "73648/90000 [=======================&gt;......] - ETA: 11s - loss: 1.0675e-04 - accuracy: 1.0000\n",
       "73728/90000 [=======================&gt;......] - ETA: 11s - loss: 1.0678e-04 - accuracy: 1.0000\n",
       "73808/90000 [=======================&gt;......] - ETA: 11s - loss: 1.0667e-04 - accuracy: 1.0000\n",
       "73888/90000 [=======================&gt;......] - ETA: 11s - loss: 1.0657e-04 - accuracy: 1.0000\n",
       "73968/90000 [=======================&gt;......] - ETA: 11s - loss: 1.0645e-04 - accuracy: 1.0000\n",
       "74048/90000 [=======================&gt;......] - ETA: 10s - loss: 1.0634e-04 - accuracy: 1.0000\n",
       "74128/90000 [=======================&gt;......] - ETA: 10s - loss: 1.0623e-04 - accuracy: 1.0000\n",
       "74208/90000 [=======================&gt;......] - ETA: 10s - loss: 1.0611e-04 - accuracy: 1.0000\n",
       "74288/90000 [=======================&gt;......] - ETA: 10s - loss: 1.0681e-04 - accuracy: 1.0000\n",
       "74368/90000 [=======================&gt;......] - ETA: 10s - loss: 1.0673e-04 - accuracy: 1.0000\n",
       "74448/90000 [=======================&gt;......] - ETA: 10s - loss: 1.0661e-04 - accuracy: 1.0000\n",
       "74528/90000 [=======================&gt;......] - ETA: 10s - loss: 1.0651e-04 - accuracy: 1.0000\n",
       "74608/90000 [=======================&gt;......] - ETA: 10s - loss: 1.0640e-04 - accuracy: 1.0000\n",
       "74688/90000 [=======================&gt;......] - ETA: 10s - loss: 1.0629e-04 - accuracy: 1.0000\n",
       "74768/90000 [=======================&gt;......] - ETA: 10s - loss: 1.0618e-04 - accuracy: 1.0000\n",
       "74848/90000 [=======================&gt;......] - ETA: 10s - loss: 1.0607e-04 - accuracy: 1.0000\n",
       "74928/90000 [=======================&gt;......] - ETA: 10s - loss: 1.0596e-04 - accuracy: 1.0000\n",
       "75008/90000 [========================&gt;.....] - ETA: 10s - loss: 1.0585e-04 - accuracy: 1.0000\n",
       "75088/90000 [========================&gt;.....] - ETA: 10s - loss: 1.0574e-04 - accuracy: 1.0000\n",
       "75168/90000 [========================&gt;.....] - ETA: 10s - loss: 1.0563e-04 - accuracy: 1.0000\n",
       "75248/90000 [========================&gt;.....] - ETA: 10s - loss: 1.0552e-04 - accuracy: 1.0000\n",
       "75328/90000 [========================&gt;.....] - ETA: 10s - loss: 1.0541e-04 - accuracy: 1.0000\n",
       "75408/90000 [========================&gt;.....] - ETA: 10s - loss: 1.0531e-04 - accuracy: 1.0000\n",
       "75488/90000 [========================&gt;.....] - ETA: 9s - loss: 1.0520e-04 - accuracy: 1.0000 \n",
       "75568/90000 [========================&gt;.....] - ETA: 9s - loss: 1.0509e-04 - accuracy: 1.0000\n",
       "75632/90000 [========================&gt;.....] - ETA: 9s - loss: 1.0501e-04 - accuracy: 1.0000\n",
       "75712/90000 [========================&gt;.....] - ETA: 9s - loss: 1.0862e-04 - accuracy: 1.0000\n",
       "75792/90000 [========================&gt;.....] - ETA: 9s - loss: 1.0851e-04 - accuracy: 1.0000\n",
       "75872/90000 [========================&gt;.....] - ETA: 9s - loss: 1.0846e-04 - accuracy: 1.0000\n",
       "75952/90000 [========================&gt;.....] - ETA: 9s - loss: 1.0835e-04 - accuracy: 1.0000\n",
       "76032/90000 [========================&gt;.....] - ETA: 9s - loss: 1.0827e-04 - accuracy: 1.0000\n",
       "76112/90000 [========================&gt;.....] - ETA: 9s - loss: 1.0826e-04 - accuracy: 1.0000\n",
       "76192/90000 [========================&gt;.....] - ETA: 9s - loss: 1.0818e-04 - accuracy: 1.0000\n",
       "76272/90000 [========================&gt;.....] - ETA: 9s - loss: 1.0838e-04 - accuracy: 1.0000\n",
       "76352/90000 [========================&gt;.....] - ETA: 9s - loss: 1.0827e-04 - accuracy: 1.0000\n",
       "76432/90000 [========================&gt;.....] - ETA: 9s - loss: 1.0816e-04 - accuracy: 1.0000\n",
       "76512/90000 [========================&gt;.....] - ETA: 9s - loss: 1.0807e-04 - accuracy: 1.0000\n",
       "76592/90000 [========================&gt;.....] - ETA: 9s - loss: 1.0795e-04 - accuracy: 1.0000\n",
       "76672/90000 [========================&gt;.....] - ETA: 9s - loss: 1.0838e-04 - accuracy: 1.0000\n",
       "76752/90000 [========================&gt;.....] - ETA: 9s - loss: 1.0827e-04 - accuracy: 1.0000\n",
       "76832/90000 [========================&gt;.....] - ETA: 9s - loss: 1.0816e-04 - accuracy: 1.0000\n",
       "76912/90000 [========================&gt;.....] - ETA: 9s - loss: 1.0806e-04 - accuracy: 1.0000\n",
       "76992/90000 [========================&gt;.....] - ETA: 8s - loss: 1.0794e-04 - accuracy: 1.0000\n",
       "77072/90000 [========================&gt;.....] - ETA: 8s - loss: 1.0783e-04 - accuracy: 1.0000\n",
       "77152/90000 [========================&gt;.....] - ETA: 8s - loss: 1.0772e-04 - accuracy: 1.0000\n",
       "77232/90000 [========================&gt;.....] - ETA: 8s - loss: 1.0762e-04 - accuracy: 1.0000\n",
       "77312/90000 [========================&gt;.....] - ETA: 8s - loss: 1.0751e-04 - accuracy: 1.0000\n",
       "77392/90000 [========================&gt;.....] - ETA: 8s - loss: 1.0739e-04 - accuracy: 1.0000\n",
       "77472/90000 [========================&gt;.....] - ETA: 8s - loss: 1.0732e-04 - accuracy: 1.0000\n",
       "77552/90000 [========================&gt;.....] - ETA: 8s - loss: 1.0721e-04 - accuracy: 1.0000\n",
       "77632/90000 [========================&gt;.....] - ETA: 8s - loss: 1.0710e-04 - accuracy: 1.0000\n",
       "77712/90000 [========================&gt;.....] - ETA: 8s - loss: 1.0700e-04 - accuracy: 1.0000\n",
       "77792/90000 [========================&gt;.....] - ETA: 8s - loss: 1.0708e-04 - accuracy: 1.0000\n",
       "77872/90000 [========================&gt;.....] - ETA: 8s - loss: 1.0697e-04 - accuracy: 1.0000\n",
       "77952/90000 [========================&gt;.....] - ETA: 8s - loss: 1.0686e-04 - accuracy: 1.0000\n",
       "78032/90000 [=========================&gt;....] - ETA: 8s - loss: 1.0676e-04 - accuracy: 1.0000\n",
       "78112/90000 [=========================&gt;....] - ETA: 8s - loss: 1.0674e-04 - accuracy: 1.0000\n",
       "78192/90000 [=========================&gt;....] - ETA: 8s - loss: 1.0664e-04 - accuracy: 1.0000\n",
       "78272/90000 [=========================&gt;....] - ETA: 8s - loss: 1.0759e-04 - accuracy: 1.0000\n",
       "78352/90000 [=========================&gt;....] - ETA: 8s - loss: 1.0753e-04 - accuracy: 1.0000\n",
       "78432/90000 [=========================&gt;....] - ETA: 7s - loss: 1.0748e-04 - accuracy: 1.0000\n",
       "78512/90000 [=========================&gt;....] - ETA: 7s - loss: 1.0737e-04 - accuracy: 1.0000\n",
       "78592/90000 [=========================&gt;....] - ETA: 7s - loss: 1.0727e-04 - accuracy: 1.0000\n",
       "78672/90000 [=========================&gt;....] - ETA: 7s - loss: 1.0716e-04 - accuracy: 1.0000\n",
       "78752/90000 [=========================&gt;....] - ETA: 7s - loss: 1.0705e-04 - accuracy: 1.0000\n",
       "78832/90000 [=========================&gt;....] - ETA: 7s - loss: 1.0694e-04 - accuracy: 1.0000\n",
       "78912/90000 [=========================&gt;....] - ETA: 7s - loss: 1.0684e-04 - accuracy: 1.0000\n",
       "78992/90000 [=========================&gt;....] - ETA: 7s - loss: 1.0678e-04 - accuracy: 1.0000\n",
       "79072/90000 [=========================&gt;....] - ETA: 7s - loss: 1.0668e-04 - accuracy: 1.0000\n",
       "79152/90000 [=========================&gt;....] - ETA: 7s - loss: 1.0657e-04 - accuracy: 1.0000\n",
       "79232/90000 [=========================&gt;....] - ETA: 7s - loss: 1.0646e-04 - accuracy: 1.0000\n",
       "79312/90000 [=========================&gt;....] - ETA: 7s - loss: 1.0636e-04 - accuracy: 1.0000\n",
       "79392/90000 [=========================&gt;....] - ETA: 7s - loss: 1.0625e-04 - accuracy: 1.0000\n",
       "79472/90000 [=========================&gt;....] - ETA: 7s - loss: 1.0615e-04 - accuracy: 1.0000\n",
       "79552/90000 [=========================&gt;....] - ETA: 7s - loss: 1.0604e-04 - accuracy: 1.0000\n",
       "79632/90000 [=========================&gt;....] - ETA: 7s - loss: 1.0594e-04 - accuracy: 1.0000\n",
       "79712/90000 [=========================&gt;....] - ETA: 7s - loss: 1.0584e-04 - accuracy: 1.0000\n",
       "79792/90000 [=========================&gt;....] - ETA: 7s - loss: 1.0575e-04 - accuracy: 1.0000\n",
       "79872/90000 [=========================&gt;....] - ETA: 6s - loss: 1.0565e-04 - accuracy: 1.0000\n",
       "79952/90000 [=========================&gt;....] - ETA: 6s - loss: 1.0554e-04 - accuracy: 1.0000\n",
       "80032/90000 [=========================&gt;....] - ETA: 6s - loss: 1.0544e-04 - accuracy: 1.0000\n",
       "80112/90000 [=========================&gt;....] - ETA: 6s - loss: 1.0534e-04 - accuracy: 1.0000\n",
       "80192/90000 [=========================&gt;....] - ETA: 6s - loss: 1.0524e-04 - accuracy: 1.0000\n",
       "80272/90000 [=========================&gt;....] - ETA: 6s - loss: 1.0513e-04 - accuracy: 1.0000\n",
       "80352/90000 [=========================&gt;....] - ETA: 6s - loss: 1.0503e-04 - accuracy: 1.0000\n",
       "80432/90000 [=========================&gt;....] - ETA: 6s - loss: 1.0492e-04 - accuracy: 1.0000\n",
       "80512/90000 [=========================&gt;....] - ETA: 6s - loss: 1.0482e-04 - accuracy: 1.0000\n",
       "80576/90000 [=========================&gt;....] - ETA: 6s - loss: 1.0474e-04 - accuracy: 1.0000\n",
       "80656/90000 [=========================&gt;....] - ETA: 6s - loss: 1.0463e-04 - accuracy: 1.0000\n",
       "80736/90000 [=========================&gt;....] - ETA: 6s - loss: 1.0453e-04 - accuracy: 1.0000\n",
       "80816/90000 [=========================&gt;....] - ETA: 6s - loss: 1.0443e-04 - accuracy: 1.0000\n",
       "80880/90000 [=========================&gt;....] - ETA: 6s - loss: 1.0435e-04 - accuracy: 1.0000\n",
       "80960/90000 [=========================&gt;....] - ETA: 6s - loss: 1.0425e-04 - accuracy: 1.0000\n",
       "81040/90000 [==========================&gt;...] - ETA: 6s - loss: 1.0415e-04 - accuracy: 1.0000\n",
       "81120/90000 [==========================&gt;...] - ETA: 6s - loss: 1.0405e-04 - accuracy: 1.0000\n",
       "81200/90000 [==========================&gt;...] - ETA: 6s - loss: 1.0394e-04 - accuracy: 1.0000\n",
       "81280/90000 [==========================&gt;...] - ETA: 5s - loss: 1.0384e-04 - accuracy: 1.0000\n",
       "81360/90000 [==========================&gt;...] - ETA: 5s - loss: 1.0374e-04 - accuracy: 1.0000\n",
       "81440/90000 [==========================&gt;...] - ETA: 5s - loss: 1.0380e-04 - accuracy: 1.0000\n",
       "81520/90000 [==========================&gt;...] - ETA: 5s - loss: 1.0370e-04 - accuracy: 1.0000\n",
       "81600/90000 [==========================&gt;...] - ETA: 5s - loss: 1.0360e-04 - accuracy: 1.0000\n",
       "81680/90000 [==========================&gt;...] - ETA: 5s - loss: 1.0350e-04 - accuracy: 1.0000\n",
       "81776/90000 [==========================&gt;...] - ETA: 5s - loss: 1.0366e-04 - accuracy: 1.0000\n",
       "81856/90000 [==========================&gt;...] - ETA: 5s - loss: 1.0356e-04 - accuracy: 1.0000\n",
       "81888/90000 [==========================&gt;...] - ETA: 5s - loss: 1.0352e-04 - accuracy: 1.0000\n",
       "81952/90000 [==========================&gt;...] - ETA: 5s - loss: 1.0344e-04 - accuracy: 1.0000\n",
       "82032/90000 [==========================&gt;...] - ETA: 5s - loss: 1.0334e-04 - accuracy: 1.0000\n",
       "82112/90000 [==========================&gt;...] - ETA: 5s - loss: 1.0324e-04 - accuracy: 1.0000\n",
       "82192/90000 [==========================&gt;...] - ETA: 5s - loss: 1.0314e-04 - accuracy: 1.0000\n",
       "82272/90000 [==========================&gt;...] - ETA: 5s - loss: 1.0304e-04 - accuracy: 1.0000\n",
       "82352/90000 [==========================&gt;...] - ETA: 5s - loss: 1.0295e-04 - accuracy: 1.0000\n",
       "82432/90000 [==========================&gt;...] - ETA: 5s - loss: 1.0285e-04 - accuracy: 1.0000\n",
       "82512/90000 [==========================&gt;...] - ETA: 5s - loss: 1.0275e-04 - accuracy: 1.0000\n",
       "82592/90000 [==========================&gt;...] - ETA: 5s - loss: 1.0265e-04 - accuracy: 1.0000\n",
       "82672/90000 [==========================&gt;...] - ETA: 5s - loss: 1.0255e-04 - accuracy: 1.0000\n",
       "82752/90000 [==========================&gt;...] - ETA: 4s - loss: 1.0250e-04 - accuracy: 1.0000\n",
       "82832/90000 [==========================&gt;...] - ETA: 4s - loss: 1.0241e-04 - accuracy: 1.0000\n",
       "82912/90000 [==========================&gt;...] - ETA: 4s - loss: 1.0231e-04 - accuracy: 1.0000\n",
       "82992/90000 [==========================&gt;...] - ETA: 4s - loss: 1.0221e-04 - accuracy: 1.0000\n",
       "83072/90000 [==========================&gt;...] - ETA: 4s - loss: 1.0212e-04 - accuracy: 1.0000\n",
       "83168/90000 [==========================&gt;...] - ETA: 4s - loss: 1.0211e-04 - accuracy: 1.0000\n",
       "83248/90000 [==========================&gt;...] - ETA: 4s - loss: 1.0205e-04 - accuracy: 1.0000\n",
       "83328/90000 [==========================&gt;...] - ETA: 4s - loss: 1.0195e-04 - accuracy: 1.0000\n",
       "83408/90000 [==========================&gt;...] - ETA: 4s - loss: 1.0187e-04 - accuracy: 1.0000\n",
       "83488/90000 [==========================&gt;...] - ETA: 4s - loss: 1.0177e-04 - accuracy: 1.0000\n",
       "83568/90000 [==========================&gt;...] - ETA: 4s - loss: 1.0167e-04 - accuracy: 1.0000\n",
       "83648/90000 [==========================&gt;...] - ETA: 4s - loss: 1.0158e-04 - accuracy: 1.0000\n",
       "83728/90000 [==========================&gt;...] - ETA: 4s - loss: 1.0148e-04 - accuracy: 1.0000\n",
       "83808/90000 [==========================&gt;...] - ETA: 4s - loss: 1.0138e-04 - accuracy: 1.0000\n",
       "83888/90000 [==========================&gt;...] - ETA: 4s - loss: 1.0129e-04 - accuracy: 1.0000\n",
       "83968/90000 [==========================&gt;...] - ETA: 4s - loss: 1.0119e-04 - accuracy: 1.0000\n",
       "84048/90000 [===========================&gt;..] - ETA: 4s - loss: 1.0118e-04 - accuracy: 1.0000\n",
       "84128/90000 [===========================&gt;..] - ETA: 4s - loss: 1.0108e-04 - accuracy: 1.0000\n",
       "84208/90000 [===========================&gt;..] - ETA: 3s - loss: 1.0099e-04 - accuracy: 1.0000\n",
       "84288/90000 [===========================&gt;..] - ETA: 3s - loss: 1.0089e-04 - accuracy: 1.0000\n",
       "84368/90000 [===========================&gt;..] - ETA: 3s - loss: 1.0080e-04 - accuracy: 1.0000\n",
       "84448/90000 [===========================&gt;..] - ETA: 3s - loss: 1.0070e-04 - accuracy: 1.0000\n",
       "84528/90000 [===========================&gt;..] - ETA: 3s - loss: 1.0061e-04 - accuracy: 1.0000\n",
       "84608/90000 [===========================&gt;..] - ETA: 3s - loss: 1.0052e-04 - accuracy: 1.0000\n",
       "84688/90000 [===========================&gt;..] - ETA: 3s - loss: 1.0043e-04 - accuracy: 1.0000\n",
       "84768/90000 [===========================&gt;..] - ETA: 3s - loss: 1.0034e-04 - accuracy: 1.0000\n",
       "84848/90000 [===========================&gt;..] - ETA: 3s - loss: 1.0027e-04 - accuracy: 1.0000\n",
       "84928/90000 [===========================&gt;..] - ETA: 3s - loss: 1.0018e-04 - accuracy: 1.0000\n",
       "85008/90000 [===========================&gt;..] - ETA: 3s - loss: 1.0012e-04 - accuracy: 1.0000\n",
       "85088/90000 [===========================&gt;..] - ETA: 3s - loss: 1.0003e-04 - accuracy: 1.0000\n",
       "85168/90000 [===========================&gt;..] - ETA: 3s - loss: 9.9933e-05 - accuracy: 1.0000\n",
       "85248/90000 [===========================&gt;..] - ETA: 3s - loss: 9.9840e-05 - accuracy: 1.0000\n",
       "85328/90000 [===========================&gt;..] - ETA: 3s - loss: 9.9747e-05 - accuracy: 1.0000\n",
       "85408/90000 [===========================&gt;..] - ETA: 3s - loss: 9.9654e-05 - accuracy: 1.0000\n",
       "85488/90000 [===========================&gt;..] - ETA: 3s - loss: 9.9562e-05 - accuracy: 1.0000\n",
       "85568/90000 [===========================&gt;..] - ETA: 3s - loss: 9.9470e-05 - accuracy: 1.0000\n",
       "85648/90000 [===========================&gt;..] - ETA: 2s - loss: 9.9380e-05 - accuracy: 1.0000\n",
       "85728/90000 [===========================&gt;..] - ETA: 2s - loss: 9.9288e-05 - accuracy: 1.0000\n",
       "85808/90000 [===========================&gt;..] - ETA: 2s - loss: 9.9196e-05 - accuracy: 1.0000\n",
       "85888/90000 [===========================&gt;..] - ETA: 2s - loss: 9.9106e-05 - accuracy: 1.0000\n",
       "85968/90000 [===========================&gt;..] - ETA: 2s - loss: 9.9462e-05 - accuracy: 1.0000\n",
       "86048/90000 [===========================&gt;..] - ETA: 2s - loss: 9.9372e-05 - accuracy: 1.0000\n",
       "86128/90000 [===========================&gt;..] - ETA: 2s - loss: 9.9281e-05 - accuracy: 1.0000\n",
       "86208/90000 [===========================&gt;..] - ETA: 2s - loss: 9.9191e-05 - accuracy: 1.0000\n",
       "86288/90000 [===========================&gt;..] - ETA: 2s - loss: 9.9100e-05 - accuracy: 1.0000\n",
       "86368/90000 [===========================&gt;..] - ETA: 2s - loss: 9.9009e-05 - accuracy: 1.0000\n",
       "86448/90000 [===========================&gt;..] - ETA: 2s - loss: 9.8929e-05 - accuracy: 1.0000\n",
       "86528/90000 [===========================&gt;..] - ETA: 2s - loss: 9.8838e-05 - accuracy: 1.0000\n",
       "86608/90000 [===========================&gt;..] - ETA: 2s - loss: 9.8747e-05 - accuracy: 1.0000\n",
       "86688/90000 [===========================&gt;..] - ETA: 2s - loss: 9.8657e-05 - accuracy: 1.0000\n",
       "86768/90000 [===========================&gt;..] - ETA: 2s - loss: 9.8567e-05 - accuracy: 1.0000\n",
       "86848/90000 [===========================&gt;..] - ETA: 2s - loss: 9.8503e-05 - accuracy: 1.0000\n",
       "86928/90000 [===========================&gt;..] - ETA: 2s - loss: 9.8461e-05 - accuracy: 1.0000\n",
       "87008/90000 [============================&gt;.] - ETA: 2s - loss: 9.8372e-05 - accuracy: 1.0000\n",
       "87088/90000 [============================&gt;.] - ETA: 2s - loss: 9.8296e-05 - accuracy: 1.0000\n",
       "87168/90000 [============================&gt;.] - ETA: 1s - loss: 9.8206e-05 - accuracy: 1.0000\n",
       "87248/90000 [============================&gt;.] - ETA: 1s - loss: 9.8118e-05 - accuracy: 1.0000\n",
       "87328/90000 [============================&gt;.] - ETA: 1s - loss: 9.8028e-05 - accuracy: 1.0000\n",
       "87408/90000 [============================&gt;.] - ETA: 1s - loss: 9.7940e-05 - accuracy: 1.0000\n",
       "87488/90000 [============================&gt;.] - ETA: 1s - loss: 9.7851e-05 - accuracy: 1.0000\n",
       "87568/90000 [============================&gt;.] - ETA: 1s - loss: 9.7762e-05 - accuracy: 1.0000\n",
       "87648/90000 [============================&gt;.] - ETA: 1s - loss: 9.7674e-05 - accuracy: 1.0000\n",
       "87728/90000 [============================&gt;.] - ETA: 1s - loss: 9.7586e-05 - accuracy: 1.0000\n",
       "87808/90000 [============================&gt;.] - ETA: 1s - loss: 9.7499e-05 - accuracy: 1.0000\n",
       "87888/90000 [============================&gt;.] - ETA: 1s - loss: 9.7412e-05 - accuracy: 1.0000\n",
       "87968/90000 [============================&gt;.] - ETA: 1s - loss: 9.7323e-05 - accuracy: 1.0000\n",
       "88032/90000 [============================&gt;.] - ETA: 1s - loss: 9.7253e-05 - accuracy: 1.0000\n",
       "88112/90000 [============================&gt;.] - ETA: 1s - loss: 9.7166e-05 - accuracy: 1.0000\n",
       "88192/90000 [============================&gt;.] - ETA: 1s - loss: 9.7083e-05 - accuracy: 1.0000\n",
       "88272/90000 [============================&gt;.] - ETA: 1s - loss: 9.6996e-05 - accuracy: 1.0000\n",
       "88352/90000 [============================&gt;.] - ETA: 1s - loss: 9.6908e-05 - accuracy: 1.0000\n",
       "88432/90000 [============================&gt;.] - ETA: 1s - loss: 9.6877e-05 - accuracy: 1.0000\n",
       "88512/90000 [============================&gt;.] - ETA: 1s - loss: 9.6790e-05 - accuracy: 1.0000\n",
       "88592/90000 [============================&gt;.] - ETA: 0s - loss: 9.6703e-05 - accuracy: 1.0000\n",
       "88672/90000 [============================&gt;.] - ETA: 0s - loss: 9.6617e-05 - accuracy: 1.0000\n",
       "88752/90000 [============================&gt;.] - ETA: 0s - loss: 9.6530e-05 - accuracy: 1.0000\n",
       "88832/90000 [============================&gt;.] - ETA: 0s - loss: 9.6445e-05 - accuracy: 1.0000\n",
       "88912/90000 [============================&gt;.] - ETA: 0s - loss: 9.6358e-05 - accuracy: 1.0000\n",
       "88992/90000 [============================&gt;.] - ETA: 0s - loss: 9.6272e-05 - accuracy: 1.0000\n",
       "89072/90000 [============================&gt;.] - ETA: 0s - loss: 9.6187e-05 - accuracy: 1.0000\n",
       "89152/90000 [============================&gt;.] - ETA: 0s - loss: 9.6103e-05 - accuracy: 1.0000\n",
       "89232/90000 [============================&gt;.] - ETA: 0s - loss: 9.6018e-05 - accuracy: 1.0000\n",
       "89312/90000 [============================&gt;.] - ETA: 0s - loss: 9.5933e-05 - accuracy: 1.0000\n",
       "89392/90000 [============================&gt;.] - ETA: 0s - loss: 9.5871e-05 - accuracy: 1.0000\n",
       "89472/90000 [============================&gt;.] - ETA: 0s - loss: 9.5787e-05 - accuracy: 1.0000\n",
       "89536/90000 [============================&gt;.] - ETA: 0s - loss: 9.5719e-05 - accuracy: 1.0000\n",
       "89616/90000 [============================&gt;.] - ETA: 0s - loss: 9.5635e-05 - accuracy: 1.0000\n",
       "89696/90000 [============================&gt;.] - ETA: 0s - loss: 9.5552e-05 - accuracy: 1.0000\n",
       "89776/90000 [============================&gt;.] - ETA: 0s - loss: 9.5467e-05 - accuracy: 1.0000\n",
       "89856/90000 [============================&gt;.] - ETA: 0s - loss: 9.5395e-05 - accuracy: 1.0000\n",
       "89936/90000 [============================&gt;.] - ETA: 0s - loss: 9.5312e-05 - accuracy: 1.0000\n",
       "90000/90000 [==============================] - 63s 697us/step - loss: 9.5244e-05 - accuracy: 1.0000 - val_loss: 1.7127e-07 - val_accuracy: 1.0000\n",
       "Training model completed.\n",
       "Saving model files...\n",
       "model saved in ./outputs/model folder\n",
       "history saved in ./outputs/model folder\n",
       "Saving model files completed.\n",
       "Evaluating model performance...\n",
       "\n",
       "  32/5000 [..............................] - ETA: 0s\n",
       " 544/5000 [==&gt;...........................] - ETA: 0s\n",
       "1088/5000 [=====&gt;........................] - ETA: 0s\n",
       "1600/5000 [========&gt;.....................] - ETA: 0s\n",
       "2144/5000 [===========&gt;..................] - ETA: 0s\n",
       "2656/5000 [==============&gt;...............] - ETA: 0s\n",
       "3200/5000 [==================&gt;...........] - ETA: 0s\n",
       "3744/5000 [=====================&gt;........] - ETA: 0s\n",
       "4288/5000 [========================&gt;.....] - ETA: 0s\n",
       "4864/5000 [============================&gt;.] - ETA: 0s\n",
       "5000/5000 [==============================] - 0s 96us/step\n",
       "[1.8590553856938642e-07, 1.0]\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Training model...\")\n",
    "model_save_path = \"model\"\n",
    "os.makedirs('./outputs/model', exist_ok=True)\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "  lr = 0.001\n",
    "  mlflow.log_metric('lr', lr)\n",
    "  opt = keras.optimizers.Adam(lr=lr)\n",
    "  model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "  history = model.fit(x_train, y_train,\n",
    "                      epochs=5, \n",
    "                      batch_size=16,\n",
    "                      validation_data=(x_val, y_val))\n",
    "  print(\"Training model completed.\")\n",
    "  \n",
    "  mlflow.keras.log_model(model, model_save_path)\n",
    "\n",
    "  print(\"Saving model files...\")\n",
    "  model.save('./outputs/model/model.h5')\n",
    "  print(\"model saved in ./outputs/model folder\")\n",
    "  with open(os.path.join('./outputs/model', 'history.txt'), 'w') as f:\n",
    "    f.write(str(history.history))\n",
    "  print(\"history saved in ./outputs/model folder\")\n",
    "  print(\"Saving model files completed.\")\n",
    "  \n",
    "  mlflow.log_artifact(os.path.join('./outputs/model', 'history.txt'))\n",
    "  \n",
    "  acc = history.history['accuracy']\n",
    "  val_acc = history.history['val_accuracy']\n",
    "  loss = history.history['loss']\n",
    "  val_loss = history.history['val_loss']\n",
    "\n",
    "  epochs = range(1, len(acc) + 1)\n",
    "\n",
    "  fig, axes = plt.subplots(ncols=2, figsize=(12.6, 4.8))\n",
    "\n",
    "  axes[0].plot(epochs, acc, 'bo', label='Training acc')\n",
    "  axes[0].plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "  axes[0].set_title('Training and validation accuracy')\n",
    "  axes[0].legend()\n",
    "\n",
    "  axes[1].plot(epochs, loss, 'bo', label='Training loss')\n",
    "  axes[1].plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "  axes[1].set_title('Training and validation loss')\n",
    "  axes[1].legend()\n",
    "  \n",
    "  training_results_graph = os.path.join('./outputs', 'training_results.png')\n",
    "  fig.savefig(training_results_graph)\n",
    "  mlflow.log_artifact(training_results_graph)\n",
    "  \n",
    "  print('Evaluating model performance...')\n",
    "  evaluation_metrics = model.evaluate(x_test, y_test)\n",
    "  print(evaluation_metrics)\n",
    "  mlflow.log_metric('eval_loss', evaluation_metrics[0])\n",
    "  mlflow.log_metric('eval_accuracy', evaluation_metrics[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the Experiment in Azure Machine Learning Workspace\n",
    "\n",
    "Run the cell below to list the experiment in Azure Machine Learning Workspace that you just completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>component-classifier</td><td>0231a3be-52bd-4b7d-999e-88d0bd10ffec</td><td></td><td>Completed</td><td><a href=\"https://ml.azure.com/experiments/component-classifier/runs/0231a3be-52bd-4b7d-999e-88d0bd10ffec?wsid=/subscriptions/281b526e-0f57-4142-ae7c-b89b634fd26e/resourcegroups/ODL-ml-181384/workspaces/AML-workspace-181384\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.run.Run?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list(ws.experiments[experiment_name].get_runs())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review Model Performance Metrics and Training Artifacts in Azure Machine Learning Workspace\n",
    "\n",
    "Return to the `HOL step-by step - Machine Learning` guide and follow instructions to review the model performance metrics and training artifacts in the Azure Machine Learning workspace."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "name": "Deep Learning",
  "notebookId": 341510685844596
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
